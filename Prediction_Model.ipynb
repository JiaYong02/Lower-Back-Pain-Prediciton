{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import Necessary Library"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"Dataset_spine.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"Unnamed: 13\"].dropna().values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["column_values = df[\"Unnamed: 13\"].dropna().values\n","# Initialize a list to store lines containing the \"=\" sign\n","lines_with_equals_sign = []\n","\n","# Iterate over the values of the column and split the data\n","for line in column_values:\n","    if \"=\" in line:\n","        # Split the line at \"=\", and take the part after it\n","        data = line.split(\"=\")[1].strip()\n","       # Split again at \"(\" to remove any content within parentheses\n","        data_without_parentheses = data.split(\"(\")[0].strip()\n","        # Append the processed data to the list\n","        lines_with_equals_sign.append(data_without_parentheses)\n","\n","# Print the lines containing \"=\" sign\n","for line in lines_with_equals_sign:\n","    print(line)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Rename the columns before \"class_att\" column accordingly\n","for i, data in enumerate(lines_with_equals_sign):\n","    if i < len(df.columns) - 1:  # Check if it's before \"class_att\" column\n","        df.rename(columns={df.columns[i]: data}, inplace=True)\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Drop unnamed column, and renamed the last column name to State"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Rename \"Class_att\" column to \"State\"\n","df.rename(columns={\"Class_att\": \"State\"}, inplace=True)\n","\n","# Drop the \"Unnamed: 13\" column\n","df.drop(columns=[\"Unnamed: 13\"], inplace=True, axis=1)\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['State'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.select_dtypes(include=['object']).columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.select_dtypes(include=np.number).columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X=df[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius','degree_spondylolisthesis',\n"," 'pelvic_slope','Direct_tilt','thoracic_slope','cervical_tilt','sacrum_angle','scoliosis_slope']]\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(X)\n","scaled_df = pd.DataFrame(data = scaled_data, columns = X.columns)\n","scaled_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y=df['State']\n","df=scaled_df.join(y)\n","df.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['State'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set()\n","sns.countplot(x = \"State\",data=df)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['State'].value_counts()*100.0 /len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_theme(style='darkgrid')\n","df.plot(kind='hist', subplots=True, layout=(4,3), sharex=False ,figsize=(15,12),title = \"Features Distribution\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_theme(style='darkgrid')\n","fig, ax = plt.subplots(2,3,figsize=(16,10))\n","sns.histplot(data=df,x='pelvic_incidence', hue='State', multiple=\"stack\", ax=ax[0][0])\n","sns.histplot(data=df,x='pelvic_tilt', hue='State', multiple=\"stack\", ax=ax[0][1])\n","sns.histplot(data=df,x='lumbar_lordosis_angle', hue='State', multiple=\"stack\", ax=ax[0][2])\n","sns.histplot(data=df,x='sacral_slope', hue='State', multiple=\"stack\", ax=ax[1][0]) \n","sns.histplot(data=df,x='pelvic_radius', hue='State', multiple=\"stack\", ax=ax[1][1])\n","sns.histplot(data=df,x='degree_spondylolisthesis', hue='State', multiple=\"stack\", ax=ax[1][2])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_theme(style='darkgrid')\n","df.plot(kind='density', subplots=True, layout=(4,3), sharex=False ,figsize=(15,12))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_theme(style='white')\n","fig, ax = plt.subplots(4,3,figsize=(20,25))\n","sns.set_palette('Set1')\n","\n","sns.scatterplot(data=df,ax=ax[0][0],y='pelvic_incidence', hue='State',x='pelvic_incidence')\n","sns.scatterplot(data=df,ax=ax[0][1],y='pelvic_incidence', hue='State',x='pelvic_tilt')\n","sns.scatterplot(data=df,ax=ax[0][2],y='pelvic_incidence', hue='State',x='lumbar_lordosis_angle')\n","sns.scatterplot(data=df,ax=ax[1][0],y='pelvic_incidence', hue='State',x='sacral_slope')\n","sns.scatterplot(data=df,ax=ax[1][1],y='pelvic_incidence', hue='State',x='pelvic_radius')\n","sns.scatterplot(data=df,ax=ax[1][2],y='pelvic_incidence', hue='State',x='degree_spondylolisthesis')\n","sns.scatterplot(data=df,ax=ax[2][0],y='pelvic_incidence', hue='State',x='pelvic_slope')\n","sns.scatterplot(data=df,ax=ax[2][1],y='pelvic_incidence', hue='State',x='Direct_tilt')\n","sns.scatterplot(data=df,ax=ax[2][2],y='pelvic_incidence', hue='State',x='thoracic_slope')\n","sns.scatterplot(data=df,ax=ax[3][0],y='pelvic_incidence', hue='State',x='cervical_tilt')   \n","sns.scatterplot(data=df,ax=ax[3][1],y='pelvic_incidence', hue='State',x='sacrum_angle')\n","sns.scatterplot(data=df,ax=ax[3][2],y='pelvic_incidence', hue='State',x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set_theme(style='white')\n","fig, ax = plt.subplots(4,3,figsize=(20,25))\n","\n","sns.regplot(data=df,ax=ax[0][0],y='pelvic_incidence', color=\"r\",x='pelvic_incidence')\n","sns.regplot(data=df,ax=ax[0][1],y='pelvic_incidence', color=\"r\",x='pelvic_tilt')\n","sns.regplot(data=df,ax=ax[0][2],y='pelvic_incidence', color=\"r\",x='lumbar_lordosis_angle')\n","sns.regplot(data=df,ax=ax[1][0],y='pelvic_incidence', color=\"r\",x='sacral_slope')\n","sns.regplot(data=df,ax=ax[1][1],y='pelvic_incidence', color=\"r\",x='pelvic_radius')\n","sns.regplot(data=df,ax=ax[1][2],y='pelvic_incidence', color=\"r\",x='degree_spondylolisthesis')\n","sns.regplot(data=df,ax=ax[2][0],y='pelvic_incidence', color=\"r\",x='pelvic_slope')\n","sns.regplot(data=df,ax=ax[2][1],y='pelvic_incidence', color=\"r\",x='Direct_tilt')\n","sns.regplot(data=df,ax=ax[2][2],y='pelvic_incidence', color=\"r\",x='thoracic_slope')\n","sns.regplot(data=df,ax=ax[3][0],y='pelvic_incidence', color=\"r\",x='cervical_tilt')   \n","sns.regplot(data=df,ax=ax[3][1],y='pelvic_incidence', color=\"r\",x='sacrum_angle')\n","sns.regplot(data=df,ax=ax[3][2],y='pelvic_incidence', color=\"r\",x='scoliosis_slope')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","sns.set_palette('rainbow')\n","\n","sns.scatterplot(data=df,ax=ax[0][0],y='pelvic_tilt', hue='State',x='pelvic_incidence')\n","sns.scatterplot(data=df,ax=ax[0][1],y='pelvic_tilt', hue='State',x='pelvic_tilt')\n","sns.scatterplot(data=df,ax=ax[0][2],y='pelvic_tilt', hue='State',x='lumbar_lordosis_angle')\n","sns.scatterplot(data=df,ax=ax[1][0],y='pelvic_tilt', hue='State',x='sacral_slope')\n","sns.scatterplot(data=df,ax=ax[1][1],y='pelvic_tilt', hue='State',x='pelvic_radius')\n","sns.scatterplot(data=df,ax=ax[1][2],y='pelvic_tilt', hue='State',x='degree_spondylolisthesis')\n","sns.scatterplot(data=df,ax=ax[2][0],y='pelvic_tilt', hue='State',x='pelvic_slope')\n","sns.scatterplot(data=df,ax=ax[2][1],y='pelvic_tilt', hue='State',x='Direct_tilt')\n","sns.scatterplot(data=df,ax=ax[2][2],y='pelvic_tilt', hue='State',x='thoracic_slope')\n","sns.scatterplot(data=df,ax=ax[3][0],y='pelvic_tilt', hue='State',x='cervical_tilt')   \n","sns.scatterplot(data=df,ax=ax[3][1],y='pelvic_tilt', hue='State',x='sacrum_angle')\n","sns.scatterplot(data=df,ax=ax[3][2],y='pelvic_tilt', hue='State',x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","\n","sns.regplot(data=df,ax=ax[0][0],y='pelvic_tilt', color=\"r\",x='pelvic_incidence')\n","sns.regplot(data=df,ax=ax[0][1],y='pelvic_tilt', color=\"r\",x='pelvic_tilt')\n","sns.regplot(data=df,ax=ax[0][2],y='pelvic_tilt', color=\"r\",x='lumbar_lordosis_angle')\n","sns.regplot(data=df,ax=ax[1][0],y='pelvic_tilt', color=\"r\",x='sacral_slope')\n","sns.regplot(data=df,ax=ax[1][1],y='pelvic_tilt', color=\"r\",x='pelvic_radius')\n","sns.regplot(data=df,ax=ax[1][2],y='pelvic_tilt', color=\"r\",x='degree_spondylolisthesis')\n","sns.regplot(data=df,ax=ax[2][0],y='pelvic_tilt', color=\"r\",x='pelvic_slope')\n","sns.regplot(data=df,ax=ax[2][1],y='pelvic_tilt', color=\"r\",x='Direct_tilt')\n","sns.regplot(data=df,ax=ax[2][2],y='pelvic_tilt', color=\"r\",x='thoracic_slope')\n","sns.regplot(data=df,ax=ax[3][0],y='pelvic_tilt', color=\"r\",x='cervical_tilt')   \n","sns.regplot(data=df,ax=ax[3][1],y='pelvic_tilt', color=\"r\",x='sacrum_angle')\n","sns.regplot(data=df,ax=ax[3][2],y='pelvic_tilt', color=\"r\",x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","sns.set_palette('Set3')\n","\n","sns.scatterplot(data=df,ax=ax[0][0],y='lumbar_lordosis_angle', hue='State',x='pelvic_incidence')\n","sns.scatterplot(data=df,ax=ax[0][1],y='lumbar_lordosis_angle', hue='State',x='pelvic_tilt')\n","sns.scatterplot(data=df,ax=ax[0][2],y='lumbar_lordosis_angle', hue='State',x='lumbar_lordosis_angle')\n","sns.scatterplot(data=df,ax=ax[1][0],y='lumbar_lordosis_angle', hue='State',x='sacral_slope')\n","sns.scatterplot(data=df,ax=ax[1][1],y='lumbar_lordosis_angle', hue='State',x='pelvic_radius')\n","sns.scatterplot(data=df,ax=ax[1][2],y='lumbar_lordosis_angle', hue='State',x='degree_spondylolisthesis')\n","sns.scatterplot(data=df,ax=ax[2][0],y='lumbar_lordosis_angle', hue='State',x='pelvic_slope')\n","sns.scatterplot(data=df,ax=ax[2][1],y='lumbar_lordosis_angle', hue='State',x='Direct_tilt')\n","sns.scatterplot(data=df,ax=ax[2][2],y='lumbar_lordosis_angle', hue='State',x='thoracic_slope')\n","sns.scatterplot(data=df,ax=ax[3][0],y='lumbar_lordosis_angle', hue='State',x='cervical_tilt')   \n","sns.scatterplot(data=df,ax=ax[3][1],y='lumbar_lordosis_angle', hue='State',x='sacrum_angle')\n","sns.scatterplot(data=df,ax=ax[3][2],y='lumbar_lordosis_angle', hue='State',x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","\n","sns.regplot(data=df,ax=ax[0][0],y='lumbar_lordosis_angle', color=\"g\",x='pelvic_incidence')\n","sns.regplot(data=df,ax=ax[0][1],y='lumbar_lordosis_angle', color=\"g\",x='pelvic_tilt')\n","sns.regplot(data=df,ax=ax[0][2],y='lumbar_lordosis_angle', color=\"g\",x='lumbar_lordosis_angle')\n","sns.regplot(data=df,ax=ax[1][0],y='lumbar_lordosis_angle', color=\"g\",x='sacral_slope')\n","sns.regplot(data=df,ax=ax[1][1],y='lumbar_lordosis_angle', color=\"g\",x='pelvic_radius')\n","sns.regplot(data=df,ax=ax[1][2],y='lumbar_lordosis_angle', color=\"g\",x='degree_spondylolisthesis')\n","sns.regplot(data=df,ax=ax[2][0],y='lumbar_lordosis_angle', color=\"g\",x='pelvic_slope')\n","sns.regplot(data=df,ax=ax[2][1],y='lumbar_lordosis_angle', color=\"g\",x='Direct_tilt')\n","sns.regplot(data=df,ax=ax[2][2],y='lumbar_lordosis_angle', color=\"g\",x='thoracic_slope')\n","sns.regplot(data=df,ax=ax[3][0],y='lumbar_lordosis_angle', color=\"g\",x='cervical_tilt')   \n","sns.regplot(data=df,ax=ax[3][1],y='lumbar_lordosis_angle', color=\"g\",x='sacrum_angle')\n","sns.regplot(data=df,ax=ax[3][2],y='lumbar_lordosis_angle', color=\"g\",x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","\n","sns.regplot(data=df,ax=ax[0][0],y='pelvic_radius', color=\"m\",x='pelvic_incidence')\n","sns.regplot(data=df,ax=ax[0][1],y='pelvic_radius', color=\"m\",x='pelvic_tilt')\n","sns.regplot(data=df,ax=ax[0][2],y='pelvic_radius', color=\"m\",x='lumbar_lordosis_angle')\n","sns.regplot(data=df,ax=ax[1][0],y='pelvic_radius', color=\"m\",x='sacral_slope')\n","sns.regplot(data=df,ax=ax[1][1],y='pelvic_radius', color=\"m\",x='pelvic_radius')\n","sns.regplot(data=df,ax=ax[1][2],y='pelvic_radius', color=\"m\",x='degree_spondylolisthesis')\n","sns.regplot(data=df,ax=ax[2][0],y='pelvic_radius', color=\"m\",x='pelvic_slope')\n","sns.regplot(data=df,ax=ax[2][1],y='pelvic_radius', color=\"m\",x='Direct_tilt')\n","sns.regplot(data=df,ax=ax[2][2],y='pelvic_radius', color=\"m\",x='thoracic_slope')\n","sns.regplot(data=df,ax=ax[3][0],y='pelvic_radius', color=\"m\",x='cervical_tilt')   \n","sns.regplot(data=df,ax=ax[3][1],y='pelvic_radius', color=\"m\",x='sacrum_angle')\n","sns.regplot(data=df,ax=ax[3][2],y='pelvic_radius', color=\"m\",x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","sns.set_palette('Set2')\n","\n","sns.scatterplot(data=df,ax=ax[0][0],y='degree_spondylolisthesis', hue='State',x='pelvic_incidence')\n","sns.scatterplot(data=df,ax=ax[0][1],y='degree_spondylolisthesis', hue='State',x='pelvic_tilt')\n","sns.scatterplot(data=df,ax=ax[0][2],y='degree_spondylolisthesis', hue='State',x='lumbar_lordosis_angle')\n","sns.scatterplot(data=df,ax=ax[1][0],y='degree_spondylolisthesis', hue='State',x='sacral_slope')\n","sns.scatterplot(data=df,ax=ax[1][1],y='degree_spondylolisthesis', hue='State',x='pelvic_radius')\n","sns.scatterplot(data=df,ax=ax[1][2],y='degree_spondylolisthesis', hue='State',x='degree_spondylolisthesis')\n","sns.scatterplot(data=df,ax=ax[2][0],y='degree_spondylolisthesis', hue='State',x='pelvic_slope')\n","sns.scatterplot(data=df,ax=ax[2][1],y='degree_spondylolisthesis', hue='State',x='Direct_tilt')\n","sns.scatterplot(data=df,ax=ax[2][2],y='degree_spondylolisthesis', hue='State',x='thoracic_slope')\n","sns.scatterplot(data=df,ax=ax[3][0],y='degree_spondylolisthesis', hue='State',x='cervical_tilt')   \n","sns.scatterplot(data=df,ax=ax[3][1],y='degree_spondylolisthesis', hue='State',x='sacrum_angle')\n","sns.scatterplot(data=df,ax=ax[3][2],y='degree_spondylolisthesis', hue='State',x='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","sns.regplot(data=df,ax=ax[0][0],y='degree_spondylolisthesis', color=\"g\",x='pelvic_incidence')\n","sns.regplot(data=df,ax=ax[0][1],y='degree_spondylolisthesis', color=\"g\",x='pelvic_tilt')\n","sns.regplot(data=df,ax=ax[0][2],y='degree_spondylolisthesis', color=\"g\",x='lumbar_lordosis_angle')\n","sns.regplot(data=df,ax=ax[1][0],y='degree_spondylolisthesis', color=\"g\",x='sacral_slope')\n","sns.regplot(data=df,ax=ax[1][1],y='degree_spondylolisthesis', color=\"g\",x='pelvic_radius')\n","sns.regplot(data=df,ax=ax[1][2],y='degree_spondylolisthesis', color=\"g\",x='degree_spondylolisthesis')\n","sns.regplot(data=df,ax=ax[2][0],y='degree_spondylolisthesis', color=\"g\",x='pelvic_slope')\n","sns.regplot(data=df,ax=ax[2][1],y='degree_spondylolisthesis', color=\"g\",x='Direct_tilt')\n","sns.regplot(data=df,ax=ax[2][2],y='degree_spondylolisthesis', color=\"g\",x='thoracic_slope')\n","sns.regplot(data=df,ax=ax[3][0],y='degree_spondylolisthesis', color=\"g\",x='cervical_tilt')   \n","sns.regplot(data=df,ax=ax[3][1],y='degree_spondylolisthesis', color=\"g\",x='sacrum_angle')\n","sns.regplot(data=df,ax=ax[3][2],y='degree_spondylolisthesis', color=\"g\",x='scoliosis_slope')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(4,3,figsize=(20,25))\n","sns.set_palette('Set2')\n","\n","sns.violinplot( data=df ,ax=ax[0][0], x ='State', orient='v',y ='pelvic_incidence')\n","sns.violinplot( data=df ,ax=ax[0][1], x ='State', orient='v',y ='pelvic_tilt')\n","sns.violinplot( data=df ,ax=ax[0][2], x ='State', orient='v',y ='lumbar_lordosis_angle')\n","sns.violinplot( data=df ,ax=ax[1][0], x ='State', orient='v',y ='sacral_slope')\n","sns.violinplot( data=df ,ax=ax[1][1], x ='State', orient='v',y ='pelvic_radius')\n","sns.violinplot( data=df ,ax=ax[1][2], x ='State', orient='v',y ='degree_spondylolisthesis')  \n","sns.violinplot( data=df ,ax=ax[2][0], x ='State', orient='v',y ='pelvic_slope')\n","sns.violinplot( data=df ,ax=ax[2][1], x ='State', orient='v',y ='Direct_tilt')\n","sns.violinplot( data=df ,ax=ax[2][2], x ='State', orient='v',y ='thoracic_slope')\n","sns.violinplot( data=df ,ax=ax[3][0], x ='State', orient='v',y ='cervical_tilt')\n","sns.violinplot( data=df ,ax=ax[3][1], x ='State', orient='v',y ='sacrum_angle')\n","sns.violinplot( data=df ,ax=ax[3][2], x ='State', orient='v',y ='scoliosis_slope')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.set()\n","sns.pairplot(df,hue=\"State\", palette='Set3')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_selected = df[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius',\n","                 'degree_spondylolisthesis','State',]]\n","\n","sns.set_style('white')\n","sns.pairplot(df_selected ,hue='State',palette='husl')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df.to_csv('new_preprocessed_data.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["numeric_df = df.select_dtypes(include='number') # Select only numeric columns\n","sns.heatmap(numeric_df.corr(), cmap=\"viridis\", annot=True, linewidths=.5, fmt='.1f')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_selected = df[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope',\n","                 'degree_spondylolisthesis',]]\n","\n","\n","plt.subplots(figsize=(10,10))\n","sns.heatmap(df_selected.corr(),cmap=\"viridis\", annot=True, linewidths=.5, fmt= '.2f')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaled_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe(include =['O'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SpineCondition_summary = df.groupby('State')\n","SpineCondition_summary.mean().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SpineCondition_summary.var().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SpineCondition_summary.std().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SpineCondition_summary.median().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.select_dtypes(include=['number']).cov()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.select_dtypes(include=['number']).corr()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["correlation = df.select_dtypes(include=['number']).corr()\n","relevant_features_correlation =  correlation[correlation >0.5]\n","relevant_features_correlation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(20,16))\n","sns.boxplot(data=df,palette= \"Set3\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df0=df[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','degree_spondylolisthesis',\n","        'State']]\n","\n","plt.figure(figsize=(15,8))\n","sns.set()\n","sns.boxplot(data=df0,palette= \"Set3\",orient=\"h\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["numeric_columns = df0.select_dtypes(include=['number'])\n","\n","q1 = numeric_columns.quantile(q=0.25)\n","q2 = numeric_columns.quantile(q=0.50)\n","q3 = numeric_columns.quantile(q=0.75)\n","\n","iqr = q3 - q1\n","print(iqr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Upper_limit =q3 + (1.5 * iqr)\n","Lower_limit =q1 - (1.5 * iqr)\n","\n","print(Upper_limit, '\\n')\n","print(Lower_limit)"]},{"cell_type":"markdown","metadata":{},"source":["Remove outliers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = df0[(df0[\"pelvic_incidence\"] > Lower_limit[\"pelvic_incidence\"])\n","             & (df0[\"pelvic_incidence\"] < Upper_limit[\"pelvic_incidence\"])]\n","\n","df0.shape[0] - df1.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df2 = df1[(df1[\"pelvic_tilt\"] > Lower_limit[\"pelvic_tilt\"])\n","             & (df1[\"pelvic_tilt\"] < Upper_limit[\"pelvic_tilt\"])]\n","\n","df1.shape[0] - df2.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df3 = df2[(df2[\"lumbar_lordosis_angle\"] > Lower_limit[\"lumbar_lordosis_angle\"])\n","             & (df2[\"lumbar_lordosis_angle\"] < Upper_limit[\"lumbar_lordosis_angle\"])]\n","\n","df2.shape[0] - df3.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df4 = df3[(df3[\"sacral_slope\"] > Lower_limit[\"sacral_slope\"])\n","             & (df3[\"sacral_slope\"] < Upper_limit[\"sacral_slope\"])]\n","\n","df3.shape[0] - df4.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final = df4[(df4[\"degree_spondylolisthesis\"] > Lower_limit[\"degree_spondylolisthesis\"])\n","             & (df4[\"degree_spondylolisthesis\"] < Upper_limit[\"degree_spondylolisthesis\"])]\n","\n","df4.shape[0] - df_final.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(15,8))\n","sns.boxplot(data=df_final, palette= \"Set3\",orient=\"h\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final['State'].value_counts()*100.0 /len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final.head()"]},{"cell_type":"markdown","metadata":{},"source":["convertion of categorical data to numerical data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final.select_dtypes(include=['object']).columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final['State'] =df_final['State'].map({'Abnormal': 0, 'Normal': 1}).astype(int)\n","df_final.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_final.to_csv('Cleaned.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["new_df = pd.read_csv('Cleaned.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Standard Scaler to transform distribution to a mean value of 0 and SD of 1"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["X=new_df[['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','degree_spondylolisthesis']]\n","y=new_df['State']"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pelvic_incidence</th>\n","      <th>pelvic_tilt</th>\n","      <th>lumbar_lordosis_angle</th>\n","      <th>sacral_slope</th>\n","      <th>degree_spondylolisthesis</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.264522</td>\n","      <td>0.789374</td>\n","      <td>-0.623128</td>\n","      <td>-0.180492</td>\n","      <td>-0.848969</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.251939</td>\n","      <td>-0.744388</td>\n","      <td>-1.431298</td>\n","      <td>-1.102302</td>\n","      <td>-0.656263</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.631712</td>\n","      <td>0.748351</td>\n","      <td>-0.042598</td>\n","      <td>0.312428</td>\n","      <td>-0.979979</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.661128</td>\n","      <td>1.047255</td>\n","      <td>-0.362735</td>\n","      <td>0.154280</td>\n","      <td>-0.390428</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.577818</td>\n","      <td>-0.794596</td>\n","      <td>-1.248439</td>\n","      <td>-0.213773</td>\n","      <td>-0.522121</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n","0          0.264522     0.789374              -0.623128     -0.180492   \n","1         -1.251939    -0.744388              -1.431298     -1.102302   \n","2          0.631712     0.748351              -0.042598      0.312428   \n","3          0.661128     1.047255              -0.362735      0.154280   \n","4         -0.577818    -0.794596              -1.248439     -0.213773   \n","\n","   degree_spondylolisthesis  \n","0                 -0.848969  \n","1                 -0.656263  \n","2                 -0.979979  \n","3                 -0.390428  \n","4                 -0.522121  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_scaled_data = scaler.fit_transform(X)\n","X_scaled = pd.DataFrame(data = X_scaled_data, columns = X.columns)\n","X_scaled.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Below can start for model building"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["## Support Vector Machine Model Building"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["X = X_scaled\n","y = new_df['State']"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No of rows in X =  288\n","No of rows in X_train =  230\n","No of rows in X_test =  58\n","-------------------------------\n","No of rows in y =  288\n","No of rows in y_train =  230\n","No of rows in y_test =  58\n"]}],"source":["print('No of rows in X = ',X.shape[0])\n","print('No of rows in X_train = ',X_train.shape[0])\n","print('No of rows in X_test = ',X_test.shape[0])\n","print('-------------------------------')\n","print('No of rows in y = ',y.shape[0])\n","print('No of rows in y_train = ',y_train.shape[0])\n","print('No of rows in y_test = ',y_test.shape[0])"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SVM Model:\n","Training Score: 0.8260869565217391\n","Test Score: 0.8448275862068966\n"]}],"source":["svc_classifier = SVC(kernel='linear', random_state=1)\n","\n","svcModel = svc_classifier.fit(X_train, y_train)\n","\n","svc_y_pred = svcModel.predict(X_test)\n","\n","svc_train_score = svcModel.score(X_train, y_train)\n","svc_test_score  = svcModel.score(X_test, y_test)\n","\n","print(\"SVM Model:\")\n","print (\"Training Score: {}\\nTest Score: {}\" .format(svc_train_score, svc_test_score))"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","plt.title('confusion_matrix of SVC')\n","sns.heatmap(confusion_matrix(y_test,svc_y_pred),annot=True,fmt=\"d\")\n","plt.show()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.90      0.88      0.89        40\n","           1       0.74      0.78      0.76        18\n","\n","    accuracy                           0.84        58\n","   macro avg       0.82      0.83      0.82        58\n","weighted avg       0.85      0.84      0.85        58\n","\n"]},{"data":{"text/plain":["0.8448275862068966"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(classification_report(y_test,svc_y_pred))\n","\n","from sklearn import metrics\n","metrics.accuracy_score(y_test, svc_y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## Random Forest"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Forest Classifier Model:\n","Training Score: 1.0\n","Test Score: 0.7931034482758621\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(random_state=1)\n","rf.fit(X_train, y_train)\n","\n","rf_y_pred = rf.predict(X_test)\n","rf_train_score = rf.score(X_train, y_train)\n","rf_test_score  = rf.score(X_test, y_test)\n","\n","print(\"Random Forest Classifier Model:\")\n","print (\"Training Score: {}\\nTest Score: {}\" .format(rf_train_score, rf_test_score))"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","plt.title('confusion_matrix of Random Forest Classifier')\n","sns.heatmap(confusion_matrix(y_test,rf_y_pred),annot=True,fmt=\"d\")\n","plt.show()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.88      0.85        40\n","           1       0.69      0.61      0.65        18\n","\n","    accuracy                           0.79        58\n","   macro avg       0.76      0.74      0.75        58\n","weighted avg       0.79      0.79      0.79        58\n","\n"]},{"data":{"text/plain":["0.7931034482758621"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(classification_report(y_test,rf_y_pred))\n","\n","from sklearn import metrics\n","metrics.accuracy_score(y_test, rf_y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Network"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import time\n","import copy\n","from plotly.subplots import make_subplots\n","import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (230, 5)\n","X_val: (58, 5)\n"]}],"source":["X = X_scaled\n","y = new_df['State']\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, \n","                                                  test_size=0.20,\n","                                                  random_state=6,\n","                                                  shuffle=True)\n","print(f'X_train: {X_train.shape}')\n","print(f'X_val: {X_val.shape}')\n","\n","X_train = X_train.values\n","X_val = X_val.values\n","y_train = y_train.values\n","y_val = y_val.values\n","\n","# fit on training set\n","sc = StandardScaler()\n","sc.fit(X_train)\n","\n","# Apply scaling params to both sets\n","X_train = sc.transform(X_train)\n","X_val = sc.transform(X_val)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["X_train_tensor = torch.FloatTensor(X_train)\n","X_val_tensor = torch.FloatTensor(X_val)\n","y_train_tensor = torch.FloatTensor(y_train)\n","y_val_tensor = torch.FloatTensor(y_val)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<torch.utils.data.dataset.TensorDataset object at 0x0000026C5C2DCED0>\n","tensor([[ 0.3279,  1.3533, -0.0406, -0.4659, -0.5769],\n","        [-0.0124,  1.8862, -0.2736, -1.2346, -0.7904],\n","        [-0.9020, -0.7924, -0.3564, -0.6123, -0.0644],\n","        [ 1.5845,  1.5967,  1.6802,  0.9434,  1.9008],\n","        [ 0.9968,  0.3514,  0.6604,  1.0156,  1.8326],\n","        [-1.0861,  0.0121, -0.5158, -1.3618, -0.5437],\n","        [ 1.3324,  2.4737,  1.8718,  0.0622,  2.4614],\n","        [-0.9700, -0.9089, -0.2554, -0.6218, -0.6548],\n","        [-1.6346, -1.5479, -1.3920, -1.0374, -0.9156],\n","        [ 1.8124,  1.5336, -0.2203,  1.2683,  1.1489],\n","        [ 0.9835,  1.8306,  2.0120,  0.0429,  1.2706],\n","        [-1.6313, -1.3783, -0.8003, -1.1429, -0.8549],\n","        [ 1.0281,  0.6662,  1.6595,  0.8511, -0.4452],\n","        [-0.3681,  0.5189, -1.1944, -0.7943, -0.8634],\n","        [-1.5805, -1.0824, -0.4627, -1.2708, -1.0045],\n","        [-1.0482, -0.3184, -0.6035, -1.1010, -0.8098]]) tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.])\n"]}],"source":["# Builds dataset containing ALL data points\n","train_dataset = TensorDataset(X_train_tensor,\n","                              y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor,\n","                            y_val_tensor)\n","print(train_dataset)\n","\n","# Builds a loader of each set\n","batch_size = 16\n","train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size=batch_size, \n","                          shuffle=True)\n","\n","val_loader = DataLoader(dataset=val_dataset,\n","                        batch_size=batch_size)\n","\n","type(train_loader)\n","\n","for x,y in train_loader:\n","    print(x,y)\n","    break"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Binary Classifier\n","class BinaryClassification(nn.Module):\n","    \n","    def __init__(self):\n","        super(BinaryClassification, self).__init__()\n","        \n","        # Number of input features is 12.\n","        self.layer_1 = nn.Linear(5, 64) \n","        self.layer_2 = nn.Linear(64, 64)\n","        self.layer_out = nn.Linear(64, 1) \n","        \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.1)      # dropout layer\n","        self.batchnorm1 = nn.BatchNorm1d(64)  # batch normalisation\n","        self.batchnorm2 = nn.BatchNorm1d(64) \n","        \n","    # Our Neural network\n","        \n","    def forward(self, inputs):\n","        x = self.relu(self.layer_1(inputs))\n","        x = self.batchnorm1(x)\n","        x = self.relu(self.layer_2(x))\n","        x = self.batchnorm2(x)\n","        x = self.dropout(x)\n","        x = self.layer_out(x)\n","        \n","        return x\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = BinaryClassification()\n","opt = optim.Adam(model.parameters(), lr=1e-4)\n","loss = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def train_val(model, params, verbose=False):\n","    t0 = time.time()\n","    \n","    # read parameters from dictionary\n","    epochs=params[\"epochs\"]\n","    loss_func=params[\"f_loss\"]\n","    opt=params[\"optimiser\"]\n","    train_dl=params[\"train\"]\n","    val_dl=params[\"val\"]\n","    lr_scheduler=params[\"lr_change\"]\n","    weight_path=params[\"weight_path\"]\n","    eval_funcs = params['eval_func'] # list of evaluation functions\n","    write_metric = params['write_metric']\n","    \n","    loss_history = {\"train\": [], \"val\": []} # history of loss values in each epoch\n","    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n","    best_val = 0.0\n","    \n","    tr_dict_eval = {}; te_dict_eval = {}\n","    for evals in eval_funcs:\n","        tr_dict_eval[evals] = []\n","        te_dict_eval[evals] = []\n","    \n","    for epoch in range(epochs):\n","        current_lr = get_lr(opt)\n","        \n","        # Training mode\n","        model.train()\n","        train_loss, train_metric = loss_epoch(model,\n","                                              loss_func,\n","                                              train_dl,\n","                                              eval_funcs,\n","                                              opt)\n","        \n","        # Evaluate mode\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss, val_metric = loss_epoch(model,\n","                                              loss_func,\n","                                              val_dl,\n","                                              eval_funcs)\n","            \n","        if val_metric['f1'] > best_val:\n","            # if verbose:\n","            #     print(\"saving model weights!\")\n","            best_val = val_metric['f1']\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            # torch.save(model.state_dict(), weight_path)\n","            print(f\"epoch: {epoch+1:03} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n","            print(f\"train-{write_metric}: {train_metric[write_metric]:.3f}\") \n","            print(f\"val-{write_metric}: {val_metric[write_metric]:.3f}\",'\\n')\n","                \n","        loss_history[\"train\"].append(train_loss)\n","        loss_history[\"val\"].append(val_loss)\n","        \n","        for evals in eval_funcs:\n","            tr_dict_eval[evals].append(train_metric[evals])\n","            te_dict_eval[evals].append(val_metric[evals])\n","        \n","        lr_scheduler.step(val_loss)\n","        if current_lr != get_lr(opt):\n","            model.load_state_dict(best_model_wts) \n","\n","    # load best model weights after training & return it\n","    model.load_state_dict(best_model_wts)\n","    \n","    t1 = time.time()\n","    print(f'Total Time: {t1 - t0:.3f}')\n","        \n","    return model, loss_history, {'train': tr_dict_eval, 'val': te_dict_eval}\n","\n","# Function to get the learning rate\n","def get_lr(opt):\n","    for param_group in opt.param_groups:\n","        return param_group['lr']\n","\n","# Function to compute the loss value per batch of data\n","def loss_batch(loss_func, output, target, opt=None):\n","    \n","    loss = loss_func(output, target) # get loss\n","    \n","    if(opt is not None):\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    return loss\n","\n","# Compute the loss value & performance metric for the entire dataset (epoch)\n","def loss_epoch(model,loss_func,dataset_dl,eval_funcs,opt=None):\n","    \n","    run_loss=0.0\n","    \n","    t_metric = {}; metric = {}\n","    for i in eval_funcs:\n","        t_metric[i] = 0.0\n","        \n","    # internal loop over dataset\n","    for xb, yb in dataset_dl:\n","        \n","        xb=xb.to(device) # send data to device\n","        yb=yb.to(device)\n","        y_pred  = model(xb) # make prediction\n","        \n","        # having made a prediction get the loss for the batch        \n","        loss = loss_batch(loss_func,y_pred, yb[:,None],opt=opt) \n","        \n","        for feval in eval_funcs:\n","            if(feval == 'accuracy'):\n","                t_metric[feval] += accuracy(y_pred, yb[:,None])\n","            if(feval == 'f1'):\n","                t_metric[feval] += f1(y_pred,yb[:,None])\n","            if(feval == 'recall'):\n","                t_metric[feval] += recall(y_pred,yb[:,None])\n","        \n","        run_loss += loss.item()\n","    loss=run_loss/len(dataset_dl)  # average loss value\n","    \n","    for feval in eval_funcs:\n","        temp = t_metric[feval]/len(dataset_dl)\n","        metric[feval] = temp  # average metric value\n","        \n","    return loss, metric\n","\n","def accuracy(y_pred, y_true):\n","    y_pred = torch.round(torch.sigmoid(y_pred))\n","    correct = (y_pred == y_true).float()\n","    accuracy = correct.sum() / len(correct)\n","    return accuracy.item()\n","\n","def f1(y_pred, y_true):\n","    y_pred = torch.round(torch.sigmoid(y_pred))\n","    tp = (y_pred * y_true).sum().float()\n","    tn = ((1 - y_pred) * (1 - y_true)).sum().float()\n","    fp = (y_pred * (1 - y_true)).sum().float()\n","    fn = ((1 - y_pred) * y_true).sum().float()\n","\n","    epsilon = 1e-7  # to avoid division by zero\n","    precision = tp / (tp + fp + epsilon)\n","    recall = tp / (tp + fn + epsilon)\n","    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n","\n","    return f1_score.item()\n","\n","def recall(y_pred, y_true):\n","    y_pred = torch.round(torch.sigmoid(y_pred))\n","    tp = (y_pred * y_true).sum().float()\n","    fn = ((1 - y_pred) * y_true).sum().float()\n","\n","    epsilon = 1e-7  # to avoid division by zero\n","    recall_score = tp / (tp + fn + epsilon)\n","\n","    return recall_score.item()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Cody\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]},{"name":"stdout","output_type":"stream","text":["epoch: 001 | train loss: 0.721 | val loss: 0.689\n","train-f1: 0.375\n","val-f1: 0.467 \n","\n","epoch: 002 | train loss: 0.664 | val loss: 0.668\n","train-f1: 0.507\n","val-f1: 0.615 \n","\n","epoch: 003 | train loss: 0.618 | val loss: 0.629\n","train-f1: 0.567\n","val-f1: 0.712 \n","\n","epoch: 012 | train loss: 0.520 | val loss: 0.542\n","train-f1: 0.681\n","val-f1: 0.746 \n","\n","epoch: 017 | train loss: 0.475 | val loss: 0.485\n","train-f1: 0.677\n","val-f1: 0.750 \n","\n","epoch: 020 | train loss: 0.438 | val loss: 0.483\n","train-f1: 0.726\n","val-f1: 0.759 \n","\n","epoch: 028 | train loss: 0.430 | val loss: 0.457\n","train-f1: 0.713\n","val-f1: 0.765 \n","\n","Total Time: 2.116\n"]}],"source":["params_train={\n","    \n","  \"train\": train_loader,                                # training data loader\n","    \"val\": val_loader,                                  # validation data loader\n"," \"epochs\": 30,                                         # number of iterations\n"," \"optimiser\": opt,                                      # optimiser\n"," \"lr_change\": ReduceLROnPlateau(opt,                    # learning rate adjustor on the fly\n","                                mode='min',\n","                                factor=0.5,\n","                                patience=5,verbose=0),\n"," \"f_loss\": loss,                                        # loss function\n"," \"weight_path\": \"weights.pt\",                           # save best weights to file\n"," \"eval_func\" : ['accuracy','f1','recall','precision'],  # save evaluation metric history\n"," \"write_metric\" : 'f1'                                  # output to screen metric\n","}\n","\n","nn_model,loss_hist,metric_hist=train_val(model,params_train,verbose=True)\n","epochs=params_train[\"epochs\"] "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 001 | train loss: 0.425 | val loss: 0.461\n","train-f1: 0.753\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.426 | val loss: 0.459\n","train-f1: 0.728\n","val-f1: 0.765 \n","\n","epoch: 016 | train loss: 0.419 | val loss: 0.447\n","train-f1: 0.720\n","val-f1: 0.775 \n","\n","Total Time: 2.876\n","epoch: 001 | train loss: 0.406 | val loss: 0.450\n","train-f1: 0.761\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.401 | val loss: 0.446\n","train-f1: 0.757\n","val-f1: 0.775 \n","\n","Total Time: 3.398\n","epoch: 001 | train loss: 0.390 | val loss: 0.447\n","train-f1: 0.774\n","val-f1: 0.775 \n","\n","Total Time: 2.070\n","epoch: 001 | train loss: 0.392 | val loss: 0.454\n","train-f1: 0.775\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.389 | val loss: 0.451\n","train-f1: 0.753\n","val-f1: 0.775 \n","\n","Total Time: 2.195\n","epoch: 001 | train loss: 0.434 | val loss: 0.455\n","train-f1: 0.706\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.403 | val loss: 0.444\n","train-f1: 0.761\n","val-f1: 0.775 \n","\n","Total Time: 2.515\n","epoch: 001 | train loss: 0.426 | val loss: 0.453\n","train-f1: 0.709\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.394 | val loss: 0.453\n","train-f1: 0.739\n","val-f1: 0.775 \n","\n","Total Time: 2.168\n","epoch: 001 | train loss: 0.391 | val loss: 0.452\n","train-f1: 0.756\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.416 | val loss: 0.446\n","train-f1: 0.708\n","val-f1: 0.775 \n","\n","Total Time: 2.025\n","epoch: 001 | train loss: 0.419 | val loss: 0.439\n","train-f1: 0.758\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.408 | val loss: 0.441\n","train-f1: 0.710\n","val-f1: 0.775 \n","\n","Total Time: 2.205\n","epoch: 001 | train loss: 0.394 | val loss: 0.446\n","train-f1: 0.754\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.398 | val loss: 0.450\n","train-f1: 0.755\n","val-f1: 0.765 \n","\n","epoch: 007 | train loss: 0.403 | val loss: 0.445\n","train-f1: 0.756\n","val-f1: 0.775 \n","\n","Total Time: 2.268\n","epoch: 001 | train loss: 0.396 | val loss: 0.451\n","train-f1: 0.758\n","val-f1: 0.765 \n","\n","epoch: 002 | train loss: 0.449 | val loss: 0.451\n","train-f1: 0.685\n","val-f1: 0.775 \n","\n","Total Time: 2.254\n","epoch: 001 | train loss: 0.419 | val loss: 0.449\n","train-f1: 0.756\n","val-f1: 0.754 \n","\n","epoch: 006 | train loss: 0.411 | val loss: 0.454\n","train-f1: 0.728\n","val-f1: 0.765 \n","\n","epoch: 009 | train loss: 0.430 | val loss: 0.447\n","train-f1: 0.746\n","val-f1: 0.775 \n","\n","Total Time: 2.008\n","epoch: 001 | train loss: 0.416 | val loss: 0.442\n","train-f1: 0.761\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.425 | val loss: 0.449\n","train-f1: 0.727\n","val-f1: 0.765 \n","\n","epoch: 013 | train loss: 0.423 | val loss: 0.452\n","train-f1: 0.712\n","val-f1: 0.775 \n","\n","Total Time: 1.950\n","epoch: 001 | train loss: 0.410 | val loss: 0.448\n","train-f1: 0.761\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.386 | val loss: 0.446\n","train-f1: 0.793\n","val-f1: 0.775 \n","\n","Total Time: 1.856\n","epoch: 001 | train loss: 0.414 | val loss: 0.449\n","train-f1: 0.725\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.395 | val loss: 0.449\n","train-f1: 0.765\n","val-f1: 0.775 \n","\n","Total Time: 1.896\n","epoch: 001 | train loss: 0.401 | val loss: 0.446\n","train-f1: 0.725\n","val-f1: 0.775 \n","\n","Total Time: 2.123\n","epoch: 001 | train loss: 0.408 | val loss: 0.445\n","train-f1: 0.729\n","val-f1: 0.775 \n","\n","Total Time: 1.937\n","epoch: 001 | train loss: 0.433 | val loss: 0.450\n","train-f1: 0.685\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.423 | val loss: 0.443\n","train-f1: 0.701\n","val-f1: 0.775 \n","\n","Total Time: 1.784\n","epoch: 001 | train loss: 0.387 | val loss: 0.447\n","train-f1: 0.771\n","val-f1: 0.775 \n","\n","Total Time: 2.080\n","epoch: 001 | train loss: 0.411 | val loss: 0.448\n","train-f1: 0.721\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.434 | val loss: 0.445\n","train-f1: 0.743\n","val-f1: 0.775 \n","\n","Total Time: 1.875\n","epoch: 001 | train loss: 0.416 | val loss: 0.447\n","train-f1: 0.767\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.408 | val loss: 0.452\n","train-f1: 0.761\n","val-f1: 0.765 \n","\n","epoch: 008 | train loss: 0.393 | val loss: 0.448\n","train-f1: 0.781\n","val-f1: 0.775 \n","\n","Total Time: 1.897\n","epoch: 001 | train loss: 0.400 | val loss: 0.444\n","train-f1: 0.711\n","val-f1: 0.775 \n","\n","Total Time: 2.178\n","epoch: 001 | train loss: 0.390 | val loss: 0.451\n","train-f1: 0.767\n","val-f1: 0.765 \n","\n","epoch: 002 | train loss: 0.411 | val loss: 0.454\n","train-f1: 0.746\n","val-f1: 0.775 \n","\n","Total Time: 2.735\n","epoch: 001 | train loss: 0.387 | val loss: 0.444\n","train-f1: 0.740\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.400 | val loss: 0.453\n","train-f1: 0.766\n","val-f1: 0.765 \n","\n","epoch: 006 | train loss: 0.422 | val loss: 0.446\n","train-f1: 0.754\n","val-f1: 0.775 \n","\n","Total Time: 1.995\n","epoch: 001 | train loss: 0.424 | val loss: 0.445\n","train-f1: 0.737\n","val-f1: 0.775 \n","\n","Total Time: 1.900\n","epoch: 001 | train loss: 0.408 | val loss: 0.449\n","train-f1: 0.725\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.395 | val loss: 0.448\n","train-f1: 0.750\n","val-f1: 0.775 \n","\n","Total Time: 1.936\n","epoch: 001 | train loss: 0.405 | val loss: 0.452\n","train-f1: 0.733\n","val-f1: 0.765 \n","\n","epoch: 008 | train loss: 0.403 | val loss: 0.450\n","train-f1: 0.765\n","val-f1: 0.775 \n","\n","Total Time: 1.754\n","epoch: 001 | train loss: 0.408 | val loss: 0.455\n","train-f1: 0.746\n","val-f1: 0.775 \n","\n","Total Time: 1.942\n","epoch: 001 | train loss: 0.399 | val loss: 0.449\n","train-f1: 0.737\n","val-f1: 0.775 \n","\n","Total Time: 2.211\n","epoch: 001 | train loss: 0.389 | val loss: 0.453\n","train-f1: 0.747\n","val-f1: 0.775 \n","\n","Total Time: 1.777\n","epoch: 001 | train loss: 0.404 | val loss: 0.447\n","train-f1: 0.755\n","val-f1: 0.775 \n","\n","Total Time: 1.786\n","epoch: 001 | train loss: 0.397 | val loss: 0.449\n","train-f1: 0.747\n","val-f1: 0.775 \n","\n","Total Time: 2.441\n","epoch: 001 | train loss: 0.417 | val loss: 0.453\n","train-f1: 0.703\n","val-f1: 0.765 \n","\n","epoch: 019 | train loss: 0.431 | val loss: 0.449\n","train-f1: 0.690\n","val-f1: 0.775 \n","\n","Total Time: 2.485\n","epoch: 001 | train loss: 0.419 | val loss: 0.447\n","train-f1: 0.725\n","val-f1: 0.730 \n","\n","epoch: 002 | train loss: 0.430 | val loss: 0.448\n","train-f1: 0.711\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.401 | val loss: 0.447\n","train-f1: 0.763\n","val-f1: 0.765 \n","\n","epoch: 006 | train loss: 0.399 | val loss: 0.445\n","train-f1: 0.738\n","val-f1: 0.775 \n","\n","Total Time: 2.100\n","epoch: 001 | train loss: 0.438 | val loss: 0.454\n","train-f1: 0.664\n","val-f1: 0.775 \n","\n","Total Time: 2.582\n","epoch: 001 | train loss: 0.404 | val loss: 0.456\n","train-f1: 0.735\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.414 | val loss: 0.453\n","train-f1: 0.731\n","val-f1: 0.775 \n","\n","Total Time: 2.268\n","epoch: 001 | train loss: 0.391 | val loss: 0.450\n","train-f1: 0.774\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.413 | val loss: 0.451\n","train-f1: 0.733\n","val-f1: 0.765 \n","\n","epoch: 005 | train loss: 0.406 | val loss: 0.449\n","train-f1: 0.747\n","val-f1: 0.775 \n","\n","Total Time: 2.175\n","epoch: 001 | train loss: 0.399 | val loss: 0.451\n","train-f1: 0.769\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.433 | val loss: 0.455\n","train-f1: 0.679\n","val-f1: 0.765 \n","\n","epoch: 012 | train loss: 0.434 | val loss: 0.443\n","train-f1: 0.685\n","val-f1: 0.775 \n","\n","Total Time: 2.745\n","epoch: 001 | train loss: 0.437 | val loss: 0.454\n","train-f1: 0.709\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.421 | val loss: 0.448\n","train-f1: 0.736\n","val-f1: 0.775 \n","\n","Total Time: 2.314\n","epoch: 001 | train loss: 0.405 | val loss: 0.448\n","train-f1: 0.776\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.369 | val loss: 0.455\n","train-f1: 0.825\n","val-f1: 0.775 \n","\n","Total Time: 2.162\n","epoch: 001 | train loss: 0.382 | val loss: 0.448\n","train-f1: 0.767\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.405 | val loss: 0.450\n","train-f1: 0.741\n","val-f1: 0.775 \n","\n","Total Time: 3.000\n","epoch: 001 | train loss: 0.417 | val loss: 0.442\n","train-f1: 0.743\n","val-f1: 0.775 \n","\n","Total Time: 3.043\n","epoch: 001 | train loss: 0.398 | val loss: 0.447\n","train-f1: 0.727\n","val-f1: 0.775 \n","\n","Total Time: 2.719\n","epoch: 001 | train loss: 0.378 | val loss: 0.449\n","train-f1: 0.778\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.419 | val loss: 0.453\n","train-f1: 0.750\n","val-f1: 0.765 \n","\n","epoch: 005 | train loss: 0.407 | val loss: 0.447\n","train-f1: 0.735\n","val-f1: 0.775 \n","\n","Total Time: 2.746\n","epoch: 001 | train loss: 0.398 | val loss: 0.452\n","train-f1: 0.737\n","val-f1: 0.765 \n","\n","epoch: 005 | train loss: 0.402 | val loss: 0.450\n","train-f1: 0.757\n","val-f1: 0.775 \n","\n","Total Time: 2.948\n","epoch: 001 | train loss: 0.417 | val loss: 0.451\n","train-f1: 0.709\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.403 | val loss: 0.449\n","train-f1: 0.776\n","val-f1: 0.775 \n","\n","Total Time: 3.200\n","epoch: 001 | train loss: 0.394 | val loss: 0.452\n","train-f1: 0.739\n","val-f1: 0.775 \n","\n","Total Time: 2.401\n","epoch: 001 | train loss: 0.373 | val loss: 0.451\n","train-f1: 0.762\n","val-f1: 0.775 \n","\n","Total Time: 2.358\n","epoch: 001 | train loss: 0.406 | val loss: 0.450\n","train-f1: 0.719\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.413 | val loss: 0.444\n","train-f1: 0.718\n","val-f1: 0.775 \n","\n","Total Time: 2.248\n","epoch: 001 | train loss: 0.379 | val loss: 0.447\n","train-f1: 0.752\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.446 | val loss: 0.447\n","train-f1: 0.709\n","val-f1: 0.775 \n","\n","Total Time: 2.281\n","epoch: 001 | train loss: 0.416 | val loss: 0.453\n","train-f1: 0.694\n","val-f1: 0.765 \n","\n","epoch: 015 | train loss: 0.426 | val loss: 0.445\n","train-f1: 0.720\n","val-f1: 0.775 \n","\n","Total Time: 3.092\n","epoch: 001 | train loss: 0.373 | val loss: 0.454\n","train-f1: 0.758\n","val-f1: 0.765 \n","\n","epoch: 026 | train loss: 0.408 | val loss: 0.444\n","train-f1: 0.731\n","val-f1: 0.775 \n","\n","Total Time: 3.323\n","epoch: 001 | train loss: 0.376 | val loss: 0.450\n","train-f1: 0.781\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.446 | val loss: 0.451\n","train-f1: 0.699\n","val-f1: 0.765 \n","\n","epoch: 008 | train loss: 0.397 | val loss: 0.451\n","train-f1: 0.759\n","val-f1: 0.775 \n","\n","Total Time: 2.163\n","epoch: 001 | train loss: 0.409 | val loss: 0.446\n","train-f1: 0.714\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.423 | val loss: 0.447\n","train-f1: 0.731\n","val-f1: 0.765 \n","\n","epoch: 005 | train loss: 0.398 | val loss: 0.451\n","train-f1: 0.752\n","val-f1: 0.775 \n","\n","Total Time: 1.597\n","epoch: 001 | train loss: 0.401 | val loss: 0.458\n","train-f1: 0.730\n","val-f1: 0.765 \n","\n","epoch: 002 | train loss: 0.393 | val loss: 0.453\n","train-f1: 0.778\n","val-f1: 0.775 \n","\n","Total Time: 1.560\n","epoch: 001 | train loss: 0.410 | val loss: 0.443\n","train-f1: 0.713\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.405 | val loss: 0.444\n","train-f1: 0.755\n","val-f1: 0.775 \n","\n","Total Time: 1.578\n","epoch: 001 | train loss: 0.426 | val loss: 0.447\n","train-f1: 0.753\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.441 | val loss: 0.448\n","train-f1: 0.683\n","val-f1: 0.765 \n","\n","epoch: 008 | train loss: 0.394 | val loss: 0.452\n","train-f1: 0.770\n","val-f1: 0.775 \n","\n","Total Time: 1.792\n","epoch: 001 | train loss: 0.434 | val loss: 0.452\n","train-f1: 0.720\n","val-f1: 0.765 \n","\n","epoch: 002 | train loss: 0.409 | val loss: 0.447\n","train-f1: 0.771\n","val-f1: 0.775 \n","\n","Total Time: 2.143\n","epoch: 001 | train loss: 0.406 | val loss: 0.445\n","train-f1: 0.761\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.439 | val loss: 0.452\n","train-f1: 0.690\n","val-f1: 0.775 \n","\n","Total Time: 1.731\n","epoch: 001 | train loss: 0.412 | val loss: 0.457\n","train-f1: 0.751\n","val-f1: 0.765 \n","\n","epoch: 002 | train loss: 0.407 | val loss: 0.445\n","train-f1: 0.737\n","val-f1: 0.775 \n","\n","Total Time: 1.547\n","epoch: 001 | train loss: 0.399 | val loss: 0.449\n","train-f1: 0.766\n","val-f1: 0.775 \n","\n","Total Time: 1.576\n","epoch: 001 | train loss: 0.410 | val loss: 0.445\n","train-f1: 0.734\n","val-f1: 0.775 \n","\n","Total Time: 1.542\n","epoch: 001 | train loss: 0.411 | val loss: 0.449\n","train-f1: 0.754\n","val-f1: 0.775 \n","\n","Total Time: 1.503\n","epoch: 001 | train loss: 0.404 | val loss: 0.447\n","train-f1: 0.737\n","val-f1: 0.775 \n","\n","Total Time: 1.578\n","epoch: 001 | train loss: 0.387 | val loss: 0.445\n","train-f1: 0.769\n","val-f1: 0.775 \n","\n","Total Time: 1.659\n","epoch: 001 | train loss: 0.403 | val loss: 0.443\n","train-f1: 0.732\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.383 | val loss: 0.448\n","train-f1: 0.793\n","val-f1: 0.775 \n","\n","Total Time: 1.724\n","epoch: 001 | train loss: 0.400 | val loss: 0.448\n","train-f1: 0.753\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.392 | val loss: 0.442\n","train-f1: 0.795\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.420 | val loss: 0.443\n","train-f1: 0.727\n","val-f1: 0.775 \n","\n","Total Time: 1.702\n","epoch: 001 | train loss: 0.410 | val loss: 0.447\n","train-f1: 0.727\n","val-f1: 0.775 \n","\n","Total Time: 1.930\n","epoch: 001 | train loss: 0.403 | val loss: 0.451\n","train-f1: 0.703\n","val-f1: 0.765 \n","\n","epoch: 005 | train loss: 0.405 | val loss: 0.444\n","train-f1: 0.735\n","val-f1: 0.775 \n","\n","Total Time: 4.137\n","epoch: 001 | train loss: 0.407 | val loss: 0.444\n","train-f1: 0.759\n","val-f1: 0.775 \n","\n","Total Time: 2.003\n","epoch: 001 | train loss: 0.425 | val loss: 0.444\n","train-f1: 0.728\n","val-f1: 0.775 \n","\n","Total Time: 1.886\n","epoch: 001 | train loss: 0.403 | val loss: 0.445\n","train-f1: 0.755\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.415 | val loss: 0.448\n","train-f1: 0.733\n","val-f1: 0.775 \n","\n","Total Time: 2.023\n","epoch: 001 | train loss: 0.437 | val loss: 0.450\n","train-f1: 0.705\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.395 | val loss: 0.451\n","train-f1: 0.740\n","val-f1: 0.775 \n","\n","Total Time: 1.852\n","epoch: 001 | train loss: 0.424 | val loss: 0.446\n","train-f1: 0.725\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.414 | val loss: 0.445\n","train-f1: 0.723\n","val-f1: 0.775 \n","\n","Total Time: 1.765\n","epoch: 001 | train loss: 0.403 | val loss: 0.444\n","train-f1: 0.750\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.445 | val loss: 0.452\n","train-f1: 0.713\n","val-f1: 0.775 \n","\n","Total Time: 1.896\n","epoch: 001 | train loss: 0.386 | val loss: 0.449\n","train-f1: 0.776\n","val-f1: 0.775 \n","\n","Total Time: 1.798\n","epoch: 001 | train loss: 0.412 | val loss: 0.450\n","train-f1: 0.702\n","val-f1: 0.775 \n","\n","Total Time: 1.749\n","epoch: 001 | train loss: 0.414 | val loss: 0.451\n","train-f1: 0.729\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.423 | val loss: 0.449\n","train-f1: 0.726\n","val-f1: 0.775 \n","\n","Total Time: 1.799\n","epoch: 001 | train loss: 0.403 | val loss: 0.448\n","train-f1: 0.738\n","val-f1: 0.730 \n","\n","epoch: 002 | train loss: 0.408 | val loss: 0.451\n","train-f1: 0.735\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.375 | val loss: 0.448\n","train-f1: 0.761\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.406 | val loss: 0.451\n","train-f1: 0.759\n","val-f1: 0.775 \n","\n","Total Time: 2.235\n","epoch: 001 | train loss: 0.420 | val loss: 0.451\n","train-f1: 0.708\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.384 | val loss: 0.454\n","train-f1: 0.805\n","val-f1: 0.775 \n","\n","Total Time: 2.085\n","epoch: 001 | train loss: 0.392 | val loss: 0.446\n","train-f1: 0.772\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.400 | val loss: 0.445\n","train-f1: 0.769\n","val-f1: 0.775 \n","\n","Total Time: 1.888\n","epoch: 001 | train loss: 0.410 | val loss: 0.451\n","train-f1: 0.723\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.416 | val loss: 0.451\n","train-f1: 0.710\n","val-f1: 0.765 \n","\n","epoch: 023 | train loss: 0.416 | val loss: 0.444\n","train-f1: 0.729\n","val-f1: 0.775 \n","\n","Total Time: 1.819\n","epoch: 001 | train loss: 0.403 | val loss: 0.443\n","train-f1: 0.742\n","val-f1: 0.775 \n","\n","Total Time: 2.030\n","epoch: 001 | train loss: 0.391 | val loss: 0.455\n","train-f1: 0.798\n","val-f1: 0.775 \n","\n","Total Time: 1.915\n","epoch: 001 | train loss: 0.415 | val loss: 0.444\n","train-f1: 0.745\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.403 | val loss: 0.458\n","train-f1: 0.757\n","val-f1: 0.765 \n","\n","epoch: 007 | train loss: 0.435 | val loss: 0.450\n","train-f1: 0.697\n","val-f1: 0.775 \n","\n","Total Time: 1.842\n","epoch: 001 | train loss: 0.409 | val loss: 0.450\n","train-f1: 0.746\n","val-f1: 0.775 \n","\n","Total Time: 1.840\n","epoch: 001 | train loss: 0.396 | val loss: 0.452\n","train-f1: 0.747\n","val-f1: 0.765 \n","\n","epoch: 006 | train loss: 0.398 | val loss: 0.453\n","train-f1: 0.797\n","val-f1: 0.775 \n","\n","Total Time: 1.831\n","epoch: 001 | train loss: 0.414 | val loss: 0.445\n","train-f1: 0.730\n","val-f1: 0.775 \n","\n","Total Time: 1.911\n","epoch: 001 | train loss: 0.419 | val loss: 0.449\n","train-f1: 0.713\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.400 | val loss: 0.452\n","train-f1: 0.751\n","val-f1: 0.775 \n","\n","Total Time: 2.085\n","epoch: 001 | train loss: 0.395 | val loss: 0.450\n","train-f1: 0.749\n","val-f1: 0.775 \n","\n","Total Time: 1.820\n","epoch: 001 | train loss: 0.405 | val loss: 0.449\n","train-f1: 0.742\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.416 | val loss: 0.456\n","train-f1: 0.775\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.392 | val loss: 0.446\n","train-f1: 0.741\n","val-f1: 0.775 \n","\n","Total Time: 1.782\n","epoch: 001 | train loss: 0.401 | val loss: 0.447\n","train-f1: 0.755\n","val-f1: 0.775 \n","\n","Total Time: 1.909\n","epoch: 001 | train loss: 0.416 | val loss: 0.447\n","train-f1: 0.722\n","val-f1: 0.775 \n","\n","Total Time: 1.815\n","epoch: 001 | train loss: 0.409 | val loss: 0.454\n","train-f1: 0.743\n","val-f1: 0.765 \n","\n","epoch: 003 | train loss: 0.418 | val loss: 0.445\n","train-f1: 0.745\n","val-f1: 0.775 \n","\n","Total Time: 1.794\n","epoch: 001 | train loss: 0.405 | val loss: 0.448\n","train-f1: 0.751\n","val-f1: 0.775 \n","\n","Total Time: 1.996\n","epoch: 001 | train loss: 0.419 | val loss: 0.451\n","train-f1: 0.744\n","val-f1: 0.775 \n","\n","Total Time: 1.900\n","epoch: 001 | train loss: 0.395 | val loss: 0.454\n","train-f1: 0.780\n","val-f1: 0.754 \n","\n","epoch: 003 | train loss: 0.412 | val loss: 0.450\n","train-f1: 0.724\n","val-f1: 0.775 \n","\n","Total Time: 1.909\n","epoch: 001 | train loss: 0.403 | val loss: 0.457\n","train-f1: 0.702\n","val-f1: 0.765 \n","\n","epoch: 004 | train loss: 0.392 | val loss: 0.448\n","train-f1: 0.777\n","val-f1: 0.775 \n","\n","Total Time: 1.874\n","epoch: 001 | train loss: 0.401 | val loss: 0.453\n","train-f1: 0.760\n","val-f1: 0.765 \n","\n","epoch: 006 | train loss: 0.433 | val loss: 0.450\n","train-f1: 0.694\n","val-f1: 0.775 \n","\n","Total Time: 1.795\n","epoch: 001 | train loss: 0.426 | val loss: 0.446\n","train-f1: 0.686\n","val-f1: 0.754 \n","\n","epoch: 004 | train loss: 0.418 | val loss: 0.455\n","train-f1: 0.719\n","val-f1: 0.765 \n","\n","epoch: 009 | train loss: 0.400 | val loss: 0.457\n","train-f1: 0.744\n","val-f1: 0.775 \n","\n","Total Time: 2.231\n","epoch: 001 | train loss: 0.415 | val loss: 0.447\n","train-f1: 0.746\n","val-f1: 0.754 \n","\n","epoch: 002 | train loss: 0.391 | val loss: 0.447\n","train-f1: 0.786\n","val-f1: 0.775 \n","\n","Total Time: 2.479\n","Average Validation Accuracy: 0.7874999940395355\n"]}],"source":["num_runs = 100  # Number of times to run the neural network\n","val_accuracies = []  # List to store validation accuracies\n","\n","for run in range(num_runs):\n","    # Train the neural network\n","    nn_model, loss_hist, metric_hist = train_val(model, params_train, verbose=True)\n","    \n","    # Get the accuracy on the validation set\n","    val_accuracy = max(metric_hist['val']['accuracy'])\n","    val_accuracies.append(val_accuracy)\n","\n","# Calculate the average validation accuracy\n","average_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n","\n","print(\"Average Validation Accuracy:\", average_val_accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["average_val_accuracy = 0.787499999"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.83      0.75      0.79        32\n","         1.0       0.72      0.81      0.76        26\n","\n","    accuracy                           0.78        58\n","   macro avg       0.78      0.78      0.78        58\n","weighted avg       0.78      0.78      0.78        58\n","\n","Accuracy Score: 0.7758620689655172\n"]}],"source":["# Import accuracy_score from sklearn.metrics\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Lists to store true labels and predicted labels\n","true_labels = []\n","predicted_labels = []\n","\n","# Iterate over the validation dataset and make predictions\n","for inputs, labels in val_loader:\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    outputs = nn_model(inputs)\n","    predictions = torch.round(torch.sigmoid(outputs))  # Convert logits to class labels (0 or 1)\n","    true_labels.extend(labels.cpu().detach().numpy())  # Detach and append true labels\n","    predicted_labels.extend(predictions.cpu().detach().numpy())  # Detach and append predicted labels\n","\n","# Convert lists to numpy arrays\n","true_labels = np.array(true_labels)\n","predicted_labels = np.array(predicted_labels)\n","\n","# Calculate classification report\n","print(\"Classification Report:\")\n","print(classification_report(true_labels, predicted_labels))\n","\n","# Calculate accuracy score\n","acc = accuracy_score(true_labels, predicted_labels)\n","print(\"Accuracy Score:\", acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Function to plot confusion matrix\n","def plot_confusion_matrix(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Function to get model predictions\n","def get_predictions(model, dataloader):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            predictions.extend(torch.round(torch.sigmoid(outputs)).cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","    return predictions, true_labels\n","\n","# Get predictions on validation data\n","val_predictions, val_true_labels = get_predictions(nn_model, val_loader)\n","\n","# Plot confusion matrix\n","plot_confusion_matrix(val_true_labels, val_predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_res(metric_hist,name):\n","    \n","    fig = make_subplots(rows=1, cols=2,\n","                        subplot_titles=['lost_hist',f'metric_{name}'])\n","\n","    # Training Data \n","\n","    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n","                             y=loss_hist[\"train\"],\n","                              line=dict(color=\"#94D4F6\",width=2),\n","                             name='train-loss'),row=1, col=1)\n","    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n","                             y=metric_hist[\"train\"][name],\n","                             line=dict(color=\"#94D4F6\",width=2),\n","                             name=f'train-{name}'),row=1, col=2)\n","\n","    # Validation Data\n","\n","    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n","                             y=loss_hist[\"val\"],\n","                             line=dict(color=\"#454545\",width=2),\n","                             name='val-loss'),row=1, col=1)\n","    fig.add_trace(go.Scatter(x=[*range(1,epochs+1)],\n","                             y=metric_hist[\"val\"][name],\n","                             line=dict(color=\"#454545\",width=2),\n","                             name=f'val-{name}'),row=1, col=2)\n","\n","    fig.update_layout(template='plotly_white',\n","                      title='Train / Validation Data Splitting',\n","                      font=dict(family='sans-serif',size=12),\n","                      )\n","\n","    fig.update_traces({'marker_line_width':3, \n","                       'marker_line_color':\"black\",\n","                       'marker_size':8,\n","                       'opacity':1.0,\n","                       'marker':{'showscale':True,'reversescale':True, 'cmid':0, 'size':10},\n","                      })\n","\n","    fig.update_coloraxes(colorscale=\"tealgrn\")\n","    fig.update_layout(coloraxis_showscale=False)\n","    fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_res(metric_hist,'f1')"]},{"cell_type":"markdown","metadata":{},"source":["## Summary of all based model accuracy"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>SVC</td>\n","      <td>84.482759</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Random Forest Classifier</td>\n","      <td>79.310345</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Neural Network</td>\n","      <td>78.749999</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      Model   Accuracy\n","0                       SVC  84.482759\n","1  Random Forest Classifier  79.310345\n","2            Neural Network  78.749999"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["models=pd.DataFrame({'Model':['SVC','Random Forest Classifier',\"Neural Network\"], # add remaining 2 algorithm into this tuple list after \",\"\n","                    'Accuracy': [svc_test_score*100,\n","                                    rf_test_score*100,\n","                                    average_val_accuracy*100\n","                                    ] # add remaining 2 algorithm test score this tuple list after \",\"\n","                                    })\n","models.sort_values(by='Accuracy', ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Performing Cross Validation for SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","    'C': [0.1, 1, 10],           # Regularization parameter\n","    'kernel': ['linear', 'rbf']  # Kernel types\n","}\n","\n","# Create the SVM classifier\n","svc_classifier = SVC(random_state=1)\n","\n","# Perform Grid Search Cross Validation\n","grid_search = GridSearchCV(estimator=svc_classifier, param_grid=param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters and best score\n","best_params = grid_search.best_params_\n","best_score = grid_search.best_score_\n","\n","print(\"Best Parameters:\", best_params)\n","print(\"Best Score:\", best_score)"]},{"cell_type":"markdown","metadata":{},"source":["### Model Rebuild for SVM with best parameter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_svc_classifier = SVC(C=1,kernel='rbf', random_state=1)\n","\n","NewSvcModel = new_svc_classifier.fit(X_train, y_train)\n","\n","new_svc_y_pred = NewSvcModel.predict(X_test)\n","\n","new_svc_train_score = NewSvcModel.score(X_train, y_train)\n","new_svc_test_score  = NewSvcModel.score(X_test, y_test)\n","\n","print(\"SVM Model:\")\n","print (\"Training Score: {}\\nTest Score: {}\" .format(new_svc_train_score, new_svc_test_score))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(classification_report(y_test,new_svc_y_pred))\n","\n","from sklearn import metrics\n","metrics.accuracy_score(y_test, new_svc_y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["# Hypeparameter Optimization - Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","# dictionary of hyperpararmeter of random forest\n","parameter_space = {\n","    'n_estimators': [100, 200, 500],\n","    'max_depth': [4, 5, 6, 7, 8],\n","    'min_samples_split': [2, 5, 10 ,15, 20],\n","    'min_samples_leaf': [1, 2, 3, 4, 8,],\n","    'max_features': ['sqrt', 'log2'],\n","    'criterion': ['gini', 'entropy'],\n","}\n","# Create random forest classifier\n","rf = RandomForestClassifier(random_state=1)\n","\n","# Perform Grid Search Cross Validation\n","random_search = RandomizedSearchCV(estimator=rf, param_distributions=parameter_space, n_iter=100, scoring='accuracy', cv=5, random_state=1)\n","random_search.fit(X_train, y_train)\n","\n","# Get the best parameters and best score\n","best_params = random_search.best_params_\n","best_score = random_search.best_score_\n","\n","print(\"Best Parameters:\", best_params)\n","print(\"Best Score:\", best_score)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model Rebuild for Random forest with best parameter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_rf = RandomForestClassifier(random_state=1,\n","                                 n_estimators=200,\n","                                 min_samples_split=2,\n","                                 min_samples_leaf=4,\n","                                 max_features='log2',\n","                                 max_depth=6,\n","                                 criterion='gini')\n","new_rf = best_rf.fit(X_train, y_train)\n","\n","best_rf_y_pred = new_rf.predict(X_test)\n","best_rf_train_score = new_rf.score(X_train, y_train)\n","best_rf_test_score  = new_rf.score(X_test, y_test)\n","\n","print(\"Random Forest Classifier Model:\")\n","print (\"Training Score: {}\\nTest Score: {}\" .format(best_rf_train_score, best_rf_test_score))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","plt.title('confusion_matrix of Random Forest Classifier')\n","sns.heatmap(confusion_matrix(y_test,best_rf_y_pred),annot=True,fmt=\"d\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, best_rf_y_pred))\n","\n","from sklearn import metrics\n","metrics.accuracy_score(y_test, best_rf_y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## K-Fold validation "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.ensemble import RandomForestClassifier\n","import numpy as np\n","\n","# Reset index of X_train and y_train\n","X_train.reset_index(drop=True, inplace=True)\n","y_train.reset_index(drop=True, inplace=True)\n","\n","# Initialize your model\n","validation_model = RandomForestClassifier(random_state=1,\n","                                 n_estimators=200,\n","                                 min_samples_split=2,\n","                                 min_samples_leaf=4,\n","                                 max_features='log2',\n","                                 max_depth=6,\n","                                 criterion='gini')\n","\n","# Initialize KFold with desired number of folds\n","kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Create lists to store training scores and fold accuracies\n","training_scores = []\n","fold_accuracies = []\n","\n","# Perform k-fold cross-validation\n","for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train)):\n","    # Get training and validation data for this fold\n","    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n","    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","    \n","    # Train the model\n","    validation_model.fit(X_train_fold, y_train_fold)\n","    \n","    # Calculate training score for this fold\n","    train_score = validation_model.score(X_train_fold, y_train_fold)\n","    training_scores.append(train_score)\n","    \n","    # Calculate accuracy for this fold\n","    fold_accuracy = validation_model.score(X_val_fold, y_val_fold)\n","    fold_accuracies.append(fold_accuracy)\n","    \n","    # Print training score and accuracy for this fold\n","    print(f\"Fold {fold_idx + 1} - Training Score: {train_score:.4f}, Accuracy: {fold_accuracy:.4f}\")\n","\n","# Calculate and print the mean and standard deviation of training scores and fold accuracies\n","print(\"Mean Training Score:\", np.mean(training_scores))\n","print(\"Mean Fold Accuracy:\", np.mean(fold_accuracies))\n","print(\"Standard Deviation of Training Scores:\", np.std(training_scores))\n","print(\"Standard Deviation of Fold Accuracies:\", np.std(fold_accuracies))"]},{"cell_type":"markdown","metadata":{},"source":["# Neural Network Parameter Tuning"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 001 | train loss: 0.405 | val loss: 0.423\n","train-f1: 0.776\n","val-f1: 0.765 \n","\n","Total Time: 2.278\n","epoch: 001 | train loss: 0.414 | val loss: 0.424\n","train-f1: 0.773\n","val-f1: 0.717 \n","\n","epoch: 002 | train loss: 0.383 | val loss: 0.428\n","train-f1: 0.755\n","val-f1: 0.729 \n","\n","epoch: 005 | train loss: 0.369 | val loss: 0.431\n","train-f1: 0.812\n","val-f1: 0.741 \n","\n","epoch: 007 | train loss: 0.360 | val loss: 0.417\n","train-f1: 0.781\n","val-f1: 0.753 \n","\n","Total Time: 2.445\n","epoch: 001 | train loss: 0.413 | val loss: 0.418\n","train-f1: 0.774\n","val-f1: 0.699 \n","\n","epoch: 006 | train loss: 0.405 | val loss: 0.415\n","train-f1: 0.734\n","val-f1: 0.717 \n","\n","epoch: 009 | train loss: 0.353 | val loss: 0.417\n","train-f1: 0.733\n","val-f1: 0.741 \n","\n","Total Time: 0.457\n","epoch: 001 | train loss: 0.493 | val loss: 0.486\n","train-f1: 0.630\n","val-f1: 0.747 \n","\n","Total Time: 6.414\n","epoch: 001 | train loss: 0.393 | val loss: 0.425\n","train-f1: 0.697\n","val-f1: 0.741 \n","\n","epoch: 004 | train loss: 0.428 | val loss: 0.416\n","train-f1: 0.715\n","val-f1: 0.750 \n","\n","Total Time: 1.259\n","epoch: 001 | train loss: 0.343 | val loss: 0.415\n","train-f1: 0.784\n","val-f1: 0.737 \n","\n","epoch: 003 | train loss: 0.340 | val loss: 0.417\n","train-f1: 0.723\n","val-f1: 0.750 \n","\n","Total Time: 4.403\n","epoch: 001 | train loss: 0.349 | val loss: 0.415\n","train-f1: 0.811\n","val-f1: 0.737 \n","\n","epoch: 002 | train loss: 0.392 | val loss: 0.417\n","train-f1: 0.661\n","val-f1: 0.750 \n","\n","Total Time: 0.858\n","epoch: 001 | train loss: 0.347 | val loss: 0.422\n","train-f1: 0.817\n","val-f1: 0.750 \n","\n","Total Time: 1.109\n","epoch: 001 | train loss: 0.315 | val loss: 0.417\n","train-f1: 0.795\n","val-f1: 0.750 \n","\n","Total Time: 0.861\n","epoch: 001 | train loss: 0.342 | val loss: 0.412\n","train-f1: 0.758\n","val-f1: 0.723 \n","\n","epoch: 004 | train loss: 0.348 | val loss: 0.417\n","train-f1: 0.741\n","val-f1: 0.750 \n","\n","Total Time: 1.093\n","Best Hyperparameters: {'lr': 0.006541210527692739, 'batch_size': 8, 'epochs': 50, 'dropout': 0.3087407548138583}\n"]}],"source":["from skopt import gp_minimize\n","from skopt.space import Real, Integer\n","from skopt.utils import use_named_args\n","import numpy as np\n","\n","# Define a function to create a new DataLoader object with the desired batch_size\n","def create_dataloader_with_batch_size(batch_size):\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","    return train_loader\n","\n","# Define the objective function for Bayesian optimization\n","@use_named_args([\n","    Real(1e-5, 1e-2, prior='log-uniform', name='lr'),\n","    Integer(8, 64, name='batch_size'),\n","    Integer(10, 50, name='epochs'),\n","    Real(0.0, 0.5, name='dropout')\n","])\n","def objective(lr, batch_size, epochs, dropout):\n","    # Convert hyperparameters to appropriate types\n","    lr = float(lr)\n","    batch_size = int(batch_size)\n","    epochs = int(epochs)\n","    dropout = float(dropout)\n","    \n","    # Update the hyperparameters in the training parameters dictionary\n","    params_train['optimiser'] = optim.Adam(model.parameters(), lr=lr)\n","    train_loader = create_dataloader_with_batch_size(batch_size)\n","    params_train['train'] = train_loader\n","    params_train['epochs'] = epochs\n","    model.dropout.p = dropout\n","    \n","    # Train the model and return the validation accuracy\n","    nn_model, _, metric_hist = train_val(model, params_train)\n","    val_accuracy = max(metric_hist['val']['accuracy'])\n","    \n","    return -val_accuracy  # Minimize negative validation accuracy\n","\n","# Define the search space for Bayesian optimization\n","space = [\n","    Real(1e-5, 1e-2, prior='log-uniform', name='lr'),\n","    Integer(8, 64, name='batch_size'),\n","    Integer(10, 50, name='epochs'),\n","    Real(0.0, 0.5, name='dropout')\n","]\n","\n","# Perform Bayesian optimization\n","result = gp_minimize(objective, space, n_calls=10, random_state=42)\n","\n","# Retrieve the best hyperparameters\n","best_hyperparameters = dict(zip(['lr', 'batch_size', 'epochs', 'dropout'], result.x))\n","\n","print(\"Best Hyperparameters:\", best_hyperparameters)\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Cody\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]}],"source":["# Binary Classifier\n","class BinaryClassification(nn.Module):\n","    \n","    def __init__(self):\n","        super(BinaryClassification, self).__init__()\n","        \n","        # Number of input features is 12.\n","        self.layer_1 = nn.Linear(5, 64) \n","        self.layer_2 = nn.Linear(64, 64)\n","        self.layer_out = nn.Linear(64, 1) \n","        \n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.3087407548138583)      # dropout layer\n","        self.batchnorm1 = nn.BatchNorm1d(64)  # batch normalisation\n","        self.batchnorm2 = nn.BatchNorm1d(64) \n","        \n","    # Our Neural network\n","        \n","    def forward(self, inputs):\n","        x = self.relu(self.layer_1(inputs))\n","        x = self.batchnorm1(x)\n","        x = self.relu(self.layer_2(x))\n","        x = self.batchnorm2(x)\n","        x = self.dropout(x)\n","        x = self.layer_out(x)\n","        \n","        return x\n","\n","opt = optim.Adam(model.parameters(), lr=0.006541210527692739)\n","batch_size = 8\n","\n","params_train={\n","    \n","  \"train\": train_loader,                                # training data loader\n","    \"val\": val_loader,                                  # validation data loader\n"," \"epochs\": 50,                                         # number of iterations\n"," \"optimiser\": opt,                                      # optimiser\n"," \"lr_change\": ReduceLROnPlateau(opt,                    # learning rate adjustor on the fly\n","                                mode='min',\n","                                factor=0.5,\n","                                patience=5,verbose=0),\n"," \"f_loss\": loss,                                        # loss function\n"," \"weight_path\": \"weights.pt\",                           # save best weights to file\n"," \"eval_func\" : ['accuracy','f1','recall','precision'],  # save evaluation metric history\n"," \"write_metric\" : 'f1'                                  # output to screen metric\n","}"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 001 | train loss: 0.400 | val loss: 0.403\n","train-f1: 0.711\n","val-f1: 0.759 \n","\n","epoch: 003 | train loss: 0.339 | val loss: 0.403\n","train-f1: 0.748\n","val-f1: 0.769 \n","\n","Total Time: 3.003\n"]}],"source":["nn_model,loss_hist,metric_hist=train_val(model,params_train,verbose=True)\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 001 | train loss: 0.330 | val loss: 0.409\n","train-f1: 0.755\n","val-f1: 0.713 \n","\n","epoch: 046 | train loss: 0.312 | val loss: 0.404\n","train-f1: 0.765\n","val-f1: 0.769 \n","\n","Total Time: 3.237\n","epoch: 001 | train loss: 0.348 | val loss: 0.411\n","train-f1: 0.715\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.375 | val loss: 0.416\n","train-f1: 0.627\n","val-f1: 0.695 \n","\n","epoch: 008 | train loss: 0.330 | val loss: 0.416\n","train-f1: 0.779\n","val-f1: 0.708 \n","\n","epoch: 014 | train loss: 0.374 | val loss: 0.405\n","train-f1: 0.739\n","val-f1: 0.713 \n","\n","Total Time: 3.337\n","epoch: 001 | train loss: 0.346 | val loss: 0.417\n","train-f1: 0.732\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.395 | val loss: 0.408\n","train-f1: 0.673\n","val-f1: 0.713 \n","\n","Total Time: 3.571\n","epoch: 001 | train loss: 0.345 | val loss: 0.405\n","train-f1: 0.666\n","val-f1: 0.713 \n","\n","Total Time: 3.172\n","epoch: 001 | train loss: 0.340 | val loss: 0.404\n","train-f1: 0.686\n","val-f1: 0.713 \n","\n","epoch: 002 | train loss: 0.342 | val loss: 0.399\n","train-f1: 0.754\n","val-f1: 0.744 \n","\n","Total Time: 3.524\n","epoch: 001 | train loss: 0.379 | val loss: 0.408\n","train-f1: 0.674\n","val-f1: 0.713 \n","\n","epoch: 039 | train loss: 0.339 | val loss: 0.402\n","train-f1: 0.802\n","val-f1: 0.737 \n","\n","epoch: 040 | train loss: 0.327 | val loss: 0.400\n","train-f1: 0.715\n","val-f1: 0.769 \n","\n","Total Time: 3.466\n","epoch: 001 | train loss: 0.374 | val loss: 0.406\n","train-f1: 0.703\n","val-f1: 0.713 \n","\n","Total Time: 3.168\n","epoch: 001 | train loss: 0.363 | val loss: 0.408\n","train-f1: 0.696\n","val-f1: 0.713 \n","\n","Total Time: 3.803\n","epoch: 001 | train loss: 0.385 | val loss: 0.406\n","train-f1: 0.684\n","val-f1: 0.713 \n","\n","Total Time: 4.046\n","epoch: 001 | train loss: 0.348 | val loss: 0.409\n","train-f1: 0.721\n","val-f1: 0.713 \n","\n","Total Time: 4.647\n","epoch: 001 | train loss: 0.357 | val loss: 0.415\n","train-f1: 0.722\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.332 | val loss: 0.412\n","train-f1: 0.779\n","val-f1: 0.713 \n","\n","Total Time: 3.180\n","epoch: 001 | train loss: 0.340 | val loss: 0.412\n","train-f1: 0.733\n","val-f1: 0.713 \n","\n","epoch: 022 | train loss: 0.346 | val loss: 0.405\n","train-f1: 0.749\n","val-f1: 0.769 \n","\n","Total Time: 3.238\n","epoch: 001 | train loss: 0.323 | val loss: 0.412\n","train-f1: 0.771\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.353 | val loss: 0.402\n","train-f1: 0.697\n","val-f1: 0.713 \n","\n","Total Time: 3.112\n","epoch: 001 | train loss: 0.336 | val loss: 0.417\n","train-f1: 0.736\n","val-f1: 0.650 \n","\n","epoch: 002 | train loss: 0.345 | val loss: 0.426\n","train-f1: 0.743\n","val-f1: 0.674 \n","\n","epoch: 003 | train loss: 0.367 | val loss: 0.409\n","train-f1: 0.685\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.336 | val loss: 0.417\n","train-f1: 0.723\n","val-f1: 0.695 \n","\n","epoch: 005 | train loss: 0.327 | val loss: 0.404\n","train-f1: 0.696\n","val-f1: 0.713 \n","\n","Total Time: 3.410\n","epoch: 001 | train loss: 0.388 | val loss: 0.402\n","train-f1: 0.692\n","val-f1: 0.716 \n","\n","epoch: 002 | train loss: 0.361 | val loss: 0.404\n","train-f1: 0.677\n","val-f1: 0.744 \n","\n","Total Time: 3.175\n","epoch: 001 | train loss: 0.331 | val loss: 0.406\n","train-f1: 0.771\n","val-f1: 0.713 \n","\n","Total Time: 3.133\n","epoch: 001 | train loss: 0.358 | val loss: 0.406\n","train-f1: 0.661\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.354 | val loss: 0.408\n","train-f1: 0.707\n","val-f1: 0.713 \n","\n","Total Time: 3.264\n","epoch: 001 | train loss: 0.352 | val loss: 0.414\n","train-f1: 0.744\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.435 | val loss: 0.407\n","train-f1: 0.689\n","val-f1: 0.713 \n","\n","Total Time: 3.558\n","epoch: 001 | train loss: 0.345 | val loss: 0.411\n","train-f1: 0.732\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.377 | val loss: 0.409\n","train-f1: 0.687\n","val-f1: 0.713 \n","\n","Total Time: 3.472\n","epoch: 001 | train loss: 0.346 | val loss: 0.418\n","train-f1: 0.718\n","val-f1: 0.650 \n","\n","epoch: 002 | train loss: 0.338 | val loss: 0.416\n","train-f1: 0.755\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.363 | val loss: 0.418\n","train-f1: 0.754\n","val-f1: 0.695 \n","\n","epoch: 004 | train loss: 0.367 | val loss: 0.409\n","train-f1: 0.734\n","val-f1: 0.713 \n","\n","Total Time: 3.134\n","epoch: 001 | train loss: 0.331 | val loss: 0.411\n","train-f1: 0.718\n","val-f1: 0.684 \n","\n","epoch: 019 | train loss: 0.364 | val loss: 0.408\n","train-f1: 0.742\n","val-f1: 0.713 \n","\n","Total Time: 3.126\n","epoch: 001 | train loss: 0.358 | val loss: 0.418\n","train-f1: 0.721\n","val-f1: 0.650 \n","\n","epoch: 002 | train loss: 0.328 | val loss: 0.424\n","train-f1: 0.746\n","val-f1: 0.674 \n","\n","epoch: 003 | train loss: 0.364 | val loss: 0.417\n","train-f1: 0.718\n","val-f1: 0.684 \n","\n","epoch: 011 | train loss: 0.393 | val loss: 0.411\n","train-f1: 0.715\n","val-f1: 0.713 \n","\n","Total Time: 3.221\n","epoch: 001 | train loss: 0.314 | val loss: 0.410\n","train-f1: 0.755\n","val-f1: 0.713 \n","\n","Total Time: 3.308\n","epoch: 001 | train loss: 0.344 | val loss: 0.414\n","train-f1: 0.754\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.356 | val loss: 0.407\n","train-f1: 0.701\n","val-f1: 0.713 \n","\n","Total Time: 3.506\n","epoch: 001 | train loss: 0.327 | val loss: 0.404\n","train-f1: 0.777\n","val-f1: 0.713 \n","\n","Total Time: 3.129\n","epoch: 001 | train loss: 0.380 | val loss: 0.412\n","train-f1: 0.742\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.349 | val loss: 0.410\n","train-f1: 0.749\n","val-f1: 0.713 \n","\n","Total Time: 3.266\n","epoch: 001 | train loss: 0.341 | val loss: 0.411\n","train-f1: 0.774\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.370 | val loss: 0.403\n","train-f1: 0.719\n","val-f1: 0.713 \n","\n","Total Time: 3.200\n","epoch: 001 | train loss: 0.361 | val loss: 0.414\n","train-f1: 0.758\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.378 | val loss: 0.405\n","train-f1: 0.657\n","val-f1: 0.713 \n","\n","Total Time: 3.472\n","epoch: 001 | train loss: 0.336 | val loss: 0.412\n","train-f1: 0.752\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.368 | val loss: 0.415\n","train-f1: 0.712\n","val-f1: 0.695 \n","\n","epoch: 013 | train loss: 0.338 | val loss: 0.403\n","train-f1: 0.786\n","val-f1: 0.713 \n","\n","Total Time: 3.034\n","epoch: 001 | train loss: 0.352 | val loss: 0.407\n","train-f1: 0.710\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.352 | val loss: 0.416\n","train-f1: 0.699\n","val-f1: 0.695 \n","\n","epoch: 006 | train loss: 0.315 | val loss: 0.405\n","train-f1: 0.787\n","val-f1: 0.713 \n","\n","Total Time: 3.214\n","epoch: 001 | train loss: 0.366 | val loss: 0.417\n","train-f1: 0.735\n","val-f1: 0.650 \n","\n","epoch: 002 | train loss: 0.357 | val loss: 0.410\n","train-f1: 0.709\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.325 | val loss: 0.416\n","train-f1: 0.735\n","val-f1: 0.695 \n","\n","epoch: 014 | train loss: 0.351 | val loss: 0.407\n","train-f1: 0.741\n","val-f1: 0.713 \n","\n","Total Time: 3.301\n","epoch: 001 | train loss: 0.343 | val loss: 0.413\n","train-f1: 0.748\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.327 | val loss: 0.409\n","train-f1: 0.765\n","val-f1: 0.713 \n","\n","Total Time: 3.164\n","epoch: 001 | train loss: 0.309 | val loss: 0.408\n","train-f1: 0.785\n","val-f1: 0.713 \n","\n","Total Time: 3.256\n","epoch: 001 | train loss: 0.397 | val loss: 0.409\n","train-f1: 0.690\n","val-f1: 0.684 \n","\n","epoch: 005 | train loss: 0.318 | val loss: 0.410\n","train-f1: 0.774\n","val-f1: 0.713 \n","\n","Total Time: 2.975\n","epoch: 001 | train loss: 0.366 | val loss: 0.408\n","train-f1: 0.703\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.345 | val loss: 0.403\n","train-f1: 0.698\n","val-f1: 0.713 \n","\n","Total Time: 3.052\n","epoch: 001 | train loss: 0.351 | val loss: 0.406\n","train-f1: 0.694\n","val-f1: 0.713 \n","\n","Total Time: 3.420\n","epoch: 001 | train loss: 0.364 | val loss: 0.412\n","train-f1: 0.706\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.313 | val loss: 0.415\n","train-f1: 0.783\n","val-f1: 0.695 \n","\n","epoch: 016 | train loss: 0.367 | val loss: 0.409\n","train-f1: 0.703\n","val-f1: 0.713 \n","\n","epoch: 027 | train loss: 0.304 | val loss: 0.402\n","train-f1: 0.846\n","val-f1: 0.754 \n","\n","Total Time: 3.804\n","epoch: 001 | train loss: 0.315 | val loss: 0.411\n","train-f1: 0.741\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.322 | val loss: 0.416\n","train-f1: 0.760\n","val-f1: 0.695 \n","\n","epoch: 019 | train loss: 0.365 | val loss: 0.410\n","train-f1: 0.729\n","val-f1: 0.713 \n","\n","Total Time: 3.578\n","epoch: 001 | train loss: 0.358 | val loss: 0.422\n","train-f1: 0.706\n","val-f1: 0.661 \n","\n","epoch: 002 | train loss: 0.350 | val loss: 0.414\n","train-f1: 0.706\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.320 | val loss: 0.408\n","train-f1: 0.745\n","val-f1: 0.713 \n","\n","Total Time: 3.121\n","epoch: 001 | train loss: 0.326 | val loss: 0.411\n","train-f1: 0.752\n","val-f1: 0.713 \n","\n","epoch: 021 | train loss: 0.347 | val loss: 0.404\n","train-f1: 0.703\n","val-f1: 0.737 \n","\n","Total Time: 3.171\n","epoch: 001 | train loss: 0.339 | val loss: 0.415\n","train-f1: 0.759\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.377 | val loss: 0.401\n","train-f1: 0.660\n","val-f1: 0.713 \n","\n","Total Time: 3.068\n","epoch: 001 | train loss: 0.368 | val loss: 0.422\n","train-f1: 0.722\n","val-f1: 0.661 \n","\n","epoch: 003 | train loss: 0.315 | val loss: 0.418\n","train-f1: 0.765\n","val-f1: 0.684 \n","\n","epoch: 007 | train loss: 0.349 | val loss: 0.408\n","train-f1: 0.738\n","val-f1: 0.713 \n","\n","Total Time: 3.291\n","epoch: 001 | train loss: 0.327 | val loss: 0.407\n","train-f1: 0.733\n","val-f1: 0.713 \n","\n","Total Time: 3.197\n","epoch: 001 | train loss: 0.377 | val loss: 0.409\n","train-f1: 0.668\n","val-f1: 0.713 \n","\n","Total Time: 3.024\n","epoch: 001 | train loss: 0.315 | val loss: 0.411\n","train-f1: 0.772\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.357 | val loss: 0.407\n","train-f1: 0.727\n","val-f1: 0.713 \n","\n","Total Time: 3.427\n","epoch: 001 | train loss: 0.350 | val loss: 0.411\n","train-f1: 0.734\n","val-f1: 0.713 \n","\n","epoch: 042 | train loss: 0.342 | val loss: 0.406\n","train-f1: 0.764\n","val-f1: 0.737 \n","\n","Total Time: 3.316\n","epoch: 001 | train loss: 0.335 | val loss: 0.410\n","train-f1: 0.745\n","val-f1: 0.713 \n","\n","Total Time: 3.298\n","epoch: 001 | train loss: 0.367 | val loss: 0.415\n","train-f1: 0.778\n","val-f1: 0.684 \n","\n","epoch: 011 | train loss: 0.365 | val loss: 0.410\n","train-f1: 0.690\n","val-f1: 0.713 \n","\n","Total Time: 3.264\n","epoch: 001 | train loss: 0.346 | val loss: 0.417\n","train-f1: 0.732\n","val-f1: 0.695 \n","\n","epoch: 004 | train loss: 0.349 | val loss: 0.411\n","train-f1: 0.708\n","val-f1: 0.713 \n","\n","Total Time: 3.348\n","epoch: 001 | train loss: 0.352 | val loss: 0.421\n","train-f1: 0.711\n","val-f1: 0.661 \n","\n","epoch: 002 | train loss: 0.335 | val loss: 0.410\n","train-f1: 0.722\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.310 | val loss: 0.405\n","train-f1: 0.748\n","val-f1: 0.713 \n","\n","epoch: 048 | train loss: 0.407 | val loss: 0.399\n","train-f1: 0.677\n","val-f1: 0.766 \n","\n","Total Time: 3.501\n","epoch: 001 | train loss: 0.345 | val loss: 0.407\n","train-f1: 0.754\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.385 | val loss: 0.415\n","train-f1: 0.656\n","val-f1: 0.695 \n","\n","epoch: 004 | train loss: 0.361 | val loss: 0.410\n","train-f1: 0.725\n","val-f1: 0.713 \n","\n","Total Time: 3.439\n","epoch: 001 | train loss: 0.362 | val loss: 0.425\n","train-f1: 0.715\n","val-f1: 0.674 \n","\n","epoch: 002 | train loss: 0.318 | val loss: 0.415\n","train-f1: 0.742\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.357 | val loss: 0.404\n","train-f1: 0.696\n","val-f1: 0.713 \n","\n","Total Time: 2.999\n","epoch: 001 | train loss: 0.362 | val loss: 0.404\n","train-f1: 0.664\n","val-f1: 0.713 \n","\n","Total Time: 3.160\n","epoch: 001 | train loss: 0.336 | val loss: 0.409\n","train-f1: 0.737\n","val-f1: 0.684 \n","\n","epoch: 014 | train loss: 0.374 | val loss: 0.415\n","train-f1: 0.700\n","val-f1: 0.695 \n","\n","epoch: 016 | train loss: 0.344 | val loss: 0.408\n","train-f1: 0.742\n","val-f1: 0.713 \n","\n","Total Time: 3.175\n","epoch: 001 | train loss: 0.363 | val loss: 0.411\n","train-f1: 0.737\n","val-f1: 0.684 \n","\n","epoch: 009 | train loss: 0.318 | val loss: 0.407\n","train-f1: 0.808\n","val-f1: 0.713 \n","\n","Total Time: 3.928\n","epoch: 001 | train loss: 0.356 | val loss: 0.415\n","train-f1: 0.728\n","val-f1: 0.661 \n","\n","epoch: 003 | train loss: 0.384 | val loss: 0.407\n","train-f1: 0.686\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.348 | val loss: 0.417\n","train-f1: 0.720\n","val-f1: 0.695 \n","\n","epoch: 015 | train loss: 0.335 | val loss: 0.408\n","train-f1: 0.727\n","val-f1: 0.713 \n","\n","Total Time: 3.138\n","epoch: 001 | train loss: 0.378 | val loss: 0.413\n","train-f1: 0.738\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.357 | val loss: 0.410\n","train-f1: 0.706\n","val-f1: 0.713 \n","\n","Total Time: 3.025\n","epoch: 001 | train loss: 0.372 | val loss: 0.415\n","train-f1: 0.708\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.326 | val loss: 0.413\n","train-f1: 0.749\n","val-f1: 0.713 \n","\n","Total Time: 3.187\n","epoch: 001 | train loss: 0.343 | val loss: 0.418\n","train-f1: 0.731\n","val-f1: 0.661 \n","\n","epoch: 002 | train loss: 0.378 | val loss: 0.432\n","train-f1: 0.717\n","val-f1: 0.674 \n","\n","epoch: 006 | train loss: 0.340 | val loss: 0.405\n","train-f1: 0.681\n","val-f1: 0.713 \n","\n","Total Time: 3.159\n","epoch: 001 | train loss: 0.339 | val loss: 0.413\n","train-f1: 0.741\n","val-f1: 0.684 \n","\n","epoch: 007 | train loss: 0.315 | val loss: 0.417\n","train-f1: 0.815\n","val-f1: 0.695 \n","\n","epoch: 012 | train loss: 0.379 | val loss: 0.406\n","train-f1: 0.701\n","val-f1: 0.713 \n","\n","Total Time: 3.105\n","epoch: 001 | train loss: 0.329 | val loss: 0.414\n","train-f1: 0.744\n","val-f1: 0.684 \n","\n","epoch: 009 | train loss: 0.350 | val loss: 0.410\n","train-f1: 0.702\n","val-f1: 0.713 \n","\n","Total Time: 3.316\n","epoch: 001 | train loss: 0.320 | val loss: 0.420\n","train-f1: 0.793\n","val-f1: 0.661 \n","\n","epoch: 004 | train loss: 0.357 | val loss: 0.408\n","train-f1: 0.734\n","val-f1: 0.684 \n","\n","epoch: 007 | train loss: 0.348 | val loss: 0.410\n","train-f1: 0.705\n","val-f1: 0.713 \n","\n","epoch: 036 | train loss: 0.417 | val loss: 0.399\n","train-f1: 0.680\n","val-f1: 0.769 \n","\n","Total Time: 3.036\n","epoch: 001 | train loss: 0.374 | val loss: 0.409\n","train-f1: 0.726\n","val-f1: 0.713 \n","\n","Total Time: 2.966\n","epoch: 001 | train loss: 0.324 | val loss: 0.407\n","train-f1: 0.733\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.369 | val loss: 0.410\n","train-f1: 0.738\n","val-f1: 0.713 \n","\n","Total Time: 3.425\n","epoch: 001 | train loss: 0.405 | val loss: 0.416\n","train-f1: 0.730\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.364 | val loss: 0.418\n","train-f1: 0.726\n","val-f1: 0.695 \n","\n","epoch: 018 | train loss: 0.355 | val loss: 0.408\n","train-f1: 0.756\n","val-f1: 0.713 \n","\n","Total Time: 3.324\n","epoch: 001 | train loss: 0.320 | val loss: 0.414\n","train-f1: 0.775\n","val-f1: 0.695 \n","\n","epoch: 021 | train loss: 0.322 | val loss: 0.409\n","train-f1: 0.765\n","val-f1: 0.713 \n","\n","Total Time: 3.233\n","epoch: 001 | train loss: 0.344 | val loss: 0.408\n","train-f1: 0.715\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.350 | val loss: 0.408\n","train-f1: 0.710\n","val-f1: 0.713 \n","\n","Total Time: 3.217\n","epoch: 001 | train loss: 0.347 | val loss: 0.414\n","train-f1: 0.738\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.342 | val loss: 0.418\n","train-f1: 0.766\n","val-f1: 0.695 \n","\n","epoch: 012 | train loss: 0.359 | val loss: 0.404\n","train-f1: 0.716\n","val-f1: 0.713 \n","\n","Total Time: 3.165\n","epoch: 001 | train loss: 0.337 | val loss: 0.409\n","train-f1: 0.748\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.353 | val loss: 0.418\n","train-f1: 0.681\n","val-f1: 0.695 \n","\n","epoch: 005 | train loss: 0.408 | val loss: 0.402\n","train-f1: 0.671\n","val-f1: 0.713 \n","\n","Total Time: 3.244\n","epoch: 001 | train loss: 0.320 | val loss: 0.410\n","train-f1: 0.764\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.324 | val loss: 0.410\n","train-f1: 0.757\n","val-f1: 0.713 \n","\n","Total Time: 2.921\n","epoch: 001 | train loss: 0.338 | val loss: 0.409\n","train-f1: 0.762\n","val-f1: 0.713 \n","\n","Total Time: 3.111\n","epoch: 001 | train loss: 0.393 | val loss: 0.414\n","train-f1: 0.676\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.331 | val loss: 0.417\n","train-f1: 0.726\n","val-f1: 0.695 \n","\n","epoch: 007 | train loss: 0.342 | val loss: 0.404\n","train-f1: 0.685\n","val-f1: 0.713 \n","\n","Total Time: 3.449\n","epoch: 001 | train loss: 0.335 | val loss: 0.415\n","train-f1: 0.743\n","val-f1: 0.684 \n","\n","epoch: 010 | train loss: 0.366 | val loss: 0.417\n","train-f1: 0.702\n","val-f1: 0.695 \n","\n","epoch: 023 | train loss: 0.357 | val loss: 0.410\n","train-f1: 0.757\n","val-f1: 0.713 \n","\n","Total Time: 4.369\n","epoch: 001 | train loss: 0.373 | val loss: 0.409\n","train-f1: 0.748\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.322 | val loss: 0.417\n","train-f1: 0.723\n","val-f1: 0.695 \n","\n","epoch: 014 | train loss: 0.342 | val loss: 0.408\n","train-f1: 0.685\n","val-f1: 0.713 \n","\n","Total Time: 3.592\n","epoch: 001 | train loss: 0.371 | val loss: 0.412\n","train-f1: 0.713\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.301 | val loss: 0.416\n","train-f1: 0.832\n","val-f1: 0.695 \n","\n","epoch: 007 | train loss: 0.338 | val loss: 0.410\n","train-f1: 0.767\n","val-f1: 0.713 \n","\n","Total Time: 3.427\n","epoch: 001 | train loss: 0.373 | val loss: 0.415\n","train-f1: 0.688\n","val-f1: 0.650 \n","\n","epoch: 003 | train loss: 0.331 | val loss: 0.409\n","train-f1: 0.777\n","val-f1: 0.684 \n","\n","epoch: 005 | train loss: 0.315 | val loss: 0.417\n","train-f1: 0.785\n","val-f1: 0.695 \n","\n","epoch: 007 | train loss: 0.348 | val loss: 0.410\n","train-f1: 0.715\n","val-f1: 0.713 \n","\n","Total Time: 3.263\n","epoch: 001 | train loss: 0.371 | val loss: 0.409\n","train-f1: 0.670\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.317 | val loss: 0.410\n","train-f1: 0.737\n","val-f1: 0.713 \n","\n","Total Time: 3.395\n","epoch: 001 | train loss: 0.334 | val loss: 0.415\n","train-f1: 0.743\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.331 | val loss: 0.408\n","train-f1: 0.759\n","val-f1: 0.713 \n","\n","Total Time: 3.238\n","epoch: 001 | train loss: 0.337 | val loss: 0.414\n","train-f1: 0.719\n","val-f1: 0.695 \n","\n","epoch: 003 | train loss: 0.311 | val loss: 0.409\n","train-f1: 0.760\n","val-f1: 0.713 \n","\n","Total Time: 3.150\n","epoch: 001 | train loss: 0.360 | val loss: 0.420\n","train-f1: 0.761\n","val-f1: 0.695 \n","\n","epoch: 008 | train loss: 0.367 | val loss: 0.404\n","train-f1: 0.744\n","val-f1: 0.713 \n","\n","Total Time: 3.141\n","epoch: 001 | train loss: 0.321 | val loss: 0.408\n","train-f1: 0.748\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.350 | val loss: 0.416\n","train-f1: 0.732\n","val-f1: 0.695 \n","\n","epoch: 006 | train loss: 0.359 | val loss: 0.406\n","train-f1: 0.699\n","val-f1: 0.713 \n","\n","Total Time: 3.108\n","epoch: 001 | train loss: 0.316 | val loss: 0.407\n","train-f1: 0.788\n","val-f1: 0.713 \n","\n","Total Time: 3.211\n","epoch: 001 | train loss: 0.349 | val loss: 0.421\n","train-f1: 0.746\n","val-f1: 0.661 \n","\n","epoch: 003 | train loss: 0.345 | val loss: 0.404\n","train-f1: 0.762\n","val-f1: 0.713 \n","\n","Total Time: 3.564\n","epoch: 001 | train loss: 0.377 | val loss: 0.413\n","train-f1: 0.751\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.342 | val loss: 0.406\n","train-f1: 0.724\n","val-f1: 0.713 \n","\n","Total Time: 3.128\n","epoch: 001 | train loss: 0.314 | val loss: 0.410\n","train-f1: 0.800\n","val-f1: 0.684 \n","\n","epoch: 005 | train loss: 0.345 | val loss: 0.404\n","train-f1: 0.741\n","val-f1: 0.713 \n","\n","Total Time: 3.163\n","epoch: 001 | train loss: 0.347 | val loss: 0.408\n","train-f1: 0.748\n","val-f1: 0.713 \n","\n","Total Time: 3.054\n","epoch: 001 | train loss: 0.338 | val loss: 0.411\n","train-f1: 0.776\n","val-f1: 0.713 \n","\n","epoch: 035 | train loss: 0.397 | val loss: 0.400\n","train-f1: 0.716\n","val-f1: 0.729 \n","\n","Total Time: 3.255\n","epoch: 001 | train loss: 0.379 | val loss: 0.420\n","train-f1: 0.679\n","val-f1: 0.661 \n","\n","epoch: 004 | train loss: 0.305 | val loss: 0.411\n","train-f1: 0.762\n","val-f1: 0.713 \n","\n","Total Time: 4.459\n","epoch: 001 | train loss: 0.337 | val loss: 0.405\n","train-f1: 0.722\n","val-f1: 0.713 \n","\n","Total Time: 3.347\n","epoch: 001 | train loss: 0.333 | val loss: 0.417\n","train-f1: 0.761\n","val-f1: 0.684 \n","\n","epoch: 004 | train loss: 0.328 | val loss: 0.410\n","train-f1: 0.747\n","val-f1: 0.695 \n","\n","epoch: 031 | train loss: 0.327 | val loss: 0.407\n","train-f1: 0.787\n","val-f1: 0.713 \n","\n","Total Time: 2.924\n","epoch: 001 | train loss: 0.382 | val loss: 0.403\n","train-f1: 0.693\n","val-f1: 0.699 \n","\n","epoch: 002 | train loss: 0.328 | val loss: 0.410\n","train-f1: 0.719\n","val-f1: 0.713 \n","\n","Total Time: 3.521\n","epoch: 001 | train loss: 0.323 | val loss: 0.413\n","train-f1: 0.781\n","val-f1: 0.684 \n","\n","epoch: 002 | train loss: 0.364 | val loss: 0.405\n","train-f1: 0.691\n","val-f1: 0.713 \n","\n","Total Time: 3.389\n","epoch: 001 | train loss: 0.362 | val loss: 0.406\n","train-f1: 0.654\n","val-f1: 0.713 \n","\n","Total Time: 3.156\n","epoch: 001 | train loss: 0.370 | val loss: 0.409\n","train-f1: 0.742\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.411 | val loss: 0.403\n","train-f1: 0.640\n","val-f1: 0.713 \n","\n","Total Time: 3.042\n","epoch: 001 | train loss: 0.356 | val loss: 0.400\n","train-f1: 0.655\n","val-f1: 0.716 \n","\n","Total Time: 3.887\n","epoch: 001 | train loss: 0.362 | val loss: 0.404\n","train-f1: 0.800\n","val-f1: 0.713 \n","\n","Total Time: 3.110\n","epoch: 001 | train loss: 0.337 | val loss: 0.405\n","train-f1: 0.719\n","val-f1: 0.713 \n","\n","Total Time: 3.137\n","epoch: 001 | train loss: 0.353 | val loss: 0.416\n","train-f1: 0.733\n","val-f1: 0.695 \n","\n","epoch: 011 | train loss: 0.339 | val loss: 0.401\n","train-f1: 0.732\n","val-f1: 0.699 \n","\n","epoch: 012 | train loss: 0.387 | val loss: 0.405\n","train-f1: 0.680\n","val-f1: 0.713 \n","\n","Total Time: 2.932\n","epoch: 001 | train loss: 0.308 | val loss: 0.414\n","train-f1: 0.776\n","val-f1: 0.650 \n","\n","epoch: 002 | train loss: 0.411 | val loss: 0.407\n","train-f1: 0.623\n","val-f1: 0.684 \n","\n","epoch: 003 | train loss: 0.320 | val loss: 0.411\n","train-f1: 0.760\n","val-f1: 0.713 \n","\n","Total Time: 2.949\n","epoch: 001 | train loss: 0.341 | val loss: 0.420\n","train-f1: 0.731\n","val-f1: 0.661 \n","\n","epoch: 002 | train loss: 0.418 | val loss: 0.424\n","train-f1: 0.696\n","val-f1: 0.674 \n","\n","epoch: 003 | train loss: 0.344 | val loss: 0.414\n","train-f1: 0.727\n","val-f1: 0.684 \n","\n","epoch: 006 | train loss: 0.336 | val loss: 0.416\n","train-f1: 0.751\n","val-f1: 0.695 \n","\n","epoch: 010 | train loss: 0.320 | val loss: 0.407\n","train-f1: 0.761\n","val-f1: 0.713 \n","\n","Total Time: 3.237\n","New Average Validation Accuracy: 0.7746874940395355\n"]}],"source":["num_runs = 100  # Number of times to run the neural network\n","val_accuracies = []  # List to store validation accuracies\n","\n","for run in range(num_runs):\n","    # Train the neural network\n","    nn_model, loss_hist, metric_hist = train_val(model, params_train, verbose=True)\n","    \n","    # Get the accuracy on the validation set\n","    val_accuracy = max(metric_hist['val']['accuracy'])\n","    val_accuracies.append(val_accuracy)\n","\n","# Calculate the average validation accuracy\n","new_average_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n","\n","print(\"New Average Validation Accuracy:\", new_average_val_accuracy)\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Refined Neural Network Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.75      0.84      0.79        32\n","         1.0       0.77      0.65      0.71        26\n","\n","    accuracy                           0.76        58\n","   macro avg       0.76      0.75      0.75        58\n","weighted avg       0.76      0.76      0.76        58\n","\n","Accuracy Score: 0.7586206896551724\n"]}],"source":["# Import accuracy_score from sklearn.metrics\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Lists to store true labels and predicted labels\n","true_labels = []\n","predicted_labels = []\n","\n","# Iterate over the validation dataset and make predictions\n","for inputs, labels in val_loader:\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    outputs = nn_model(inputs)\n","    predictions = torch.round(torch.sigmoid(outputs))  # Convert logits to class labels (0 or 1)\n","    true_labels.extend(labels.cpu().detach().numpy())  # Detach and append true labels\n","    predicted_labels.extend(predictions.cpu().detach().numpy())  # Detach and append predicted labels\n","\n","# Convert lists to numpy arrays\n","true_labels = np.array(true_labels)\n","predicted_labels = np.array(predicted_labels)\n","\n","# Calculate classification report\n","print(\"Refined Neural Network Classification Report:\")\n","print(classification_report(true_labels, predicted_labels))\n","\n","# Calculate accuracy score\n","acc = accuracy_score(true_labels, predicted_labels)\n","print(\"Accuracy Score:\", acc)\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Validation Accuracy: 0.7874999940395355\n","New Average Validation Accuracy: 0.7746874940395355\n"]}],"source":["#Before Parameter Tuning\n","print(\"Average Validation Accuracy:\", average_val_accuracy)\n","\n","#After Parameter Tuning\n","print(\"New Average Validation Accuracy:\", new_average_val_accuracy)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["new_svc_test_score = 0.8448275862068966\n","best_rf_test_score = 0.81034483\n","new_average_val_accuracy = 0.77468749"]},{"cell_type":"markdown","metadata":{},"source":["# Refined Model Summary"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>SVC</td>\n","      <td>84.482759</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Random Forest</td>\n","      <td>81.034483</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Neural Network</td>\n","      <td>77.468749</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Model   Accuracy\n","0             SVC  84.482759\n","1   Random Forest  81.034483\n","2  Neural Network  77.468749"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["new_models=pd.DataFrame({'Model':['SVC','Random Forest','Neural Network'], # add remaining 2 algorithm into this tuple list after \",\"\n","                    'Accuracy': [new_svc_test_score*100,\n","                                 best_rf_test_score*100,\n","                                 new_average_val_accuracy*100\n","                                    ] # add remaining 2 algorithm test score this tuple list after \",\"\n","                                    })\n","new_models.sort_values(by='Accuracy', ascending=False)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import joblib\n","\n","loaded_model = joblib.load('svm_model.pkl')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["new_data_predictions = loaded_model.predict(X_test)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0\n"," 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1]\n"]}],"source":["print(new_data_predictions)"]}],"metadata":{"kernelspec":{"display_name":"JJ","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
